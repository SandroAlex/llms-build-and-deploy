{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d5f80a",
   "metadata": {},
   "source": [
    "# Looking Inside Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938e356",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc39666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 17:30:00.137398: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-15 17:30:00.141169: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-15 17:30:00.152885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765819800.172696     148 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765819800.178683     148 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765819800.193747     148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765819800.193761     148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765819800.193763     148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765819800.193765     148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 17:30:00.198564: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Initial imports\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bc30b",
   "metadata": {},
   "source": [
    "## Loading the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248da2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0682ad4e4c1f457abfaca1d97ad6edca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "device: str = \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"microsoft/Phi-3-mini-4k-instruct\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=device,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=250,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f06ee7",
   "metadata": {},
   "source": [
    "## An Overview of Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b55c98",
   "metadata": {},
   "source": [
    "### The Inputs and Outputs of a Trained Transformer LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c94952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mention the steps you're taking to prevent it in the future.\n",
      "\n",
      "Email:\n",
      "\n",
      "Subject: Sincere Apologies for the Gardening Mishap\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this email finds you well\n"
     ]
    }
   ],
   "source": [
    "prompt: str = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "output = generator(prompt, max_new_tokens=50)\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd8d430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Solution 1:**\n",
      "\n",
      "- Gary Marcus is a cognitive scientist and psychologist known for his work in the field of artificial intelligence (AI).\n",
      "- He has been a professor at New York University and is currently a researcher at Google Brain.\n",
      "- Marcus has written several books on AI, including \"Guitar Zero,\" \"The Baby Code,\" and \"Kluge: The Haphazard Construction of the Human Mind.\"\n",
      "- He has been a vocal critic of deep learning and has advocated for a more balanced approach to AI research that includes symbolic reasoning and other cognitive processes.\n",
      "- Marcus has also been involved in the development of AI technologies, such as the AI startup Geometric Intelligence and the AI startup Robust.AI.\n",
      "\n",
      "**Main ideas of Gary Marcus:**\n",
      "\n",
      "- AI research should focus on developing systems that can reason and understand the world in a more human-like way.\n",
      "- Deep learning has limitations and should be combined with other approaches to create more robust AI systems.\n",
      "- AI should be developed with ethical considerations in mind, such as avoiding bias and ensuring transparency.\n",
      "- AI research should be interdisciplinary, incorporating insights from psychology, neuroscience, and other fields.\n",
      "- AI should be developed with a focus on solving real-world problems and improving human lives.\n",
      "\n",
      "**Instruction 2 (more difficult):**\n",
      "\n",
      "Please provide a comprehensive analysis of the contributions of Gary Marcus to the field of AI, including his critiques of deep learning, his proposed solutions, and his impact on the AI community. Additionally, discuss the implications of his work for the future of AI and how it may influence the development of more human-like AI systems.\n",
      "\n",
      "**Solution 2:**\n",
      "\n",
      "Gary Marcus has made significant contributions to the field of AI, particularly in the areas of cognitive science and AI research. He has been a vocal critic of deep learning, arguing that it has limitations and should be combined with other approaches to create more robust AI systems. Marcus has proposed a more balanced approach to AI research that includes symbolic reasoning and other cognitive processes.\n",
      "\n",
      "Marcus's critiques of deep learning have sparked a debate within\n",
      "CPU times: user 10min 52s, sys: 1.21 s, total: 10min 54s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt: str = \"Please, can you give me a brief overview of life and works of Gary Marcus in the Field of AI? Give me a list of bullet points of his main ideas.\"\n",
    "output = generator(prompt, max_new_tokens=500)\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e0add",
   "metadata": {},
   "source": [
    "### The Components of the Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35996a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3SdpaAttention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cfc34c",
   "metadata": {},
   "source": [
    "### Choosing a Single Token from the Probability Distribution (Sampling / Decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440c9147",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The capital of Brazil is\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids: torch.Tensor = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids: torch.Tensor = input_ids.to(device)\n",
    "\n",
    "# Get the output of the model before the lm_head\n",
    "model_output = model.model(input_ids)\n",
    "\n",
    "# Get the output of the lm_head\n",
    "lm_head_output = model.lm_head(model_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be59fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Shape of model output before classification head: torch.Size([1, 5, 3072])\n",
      ">>> Shape of classification head output:              torch.Size([1, 5, 32064])\n"
     ]
    }
   ],
   "source": [
    "print(f\">>> Shape of model output before classification head: {model_output[0].shape}\")\n",
    "print(f\">>> Shape of classification head output:              {lm_head_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9935098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bras'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test decoding the last token. \n",
    "# Greedy decoding: grab the token with the highest logit value\n",
    "tokenizer.decode(lm_head_output[0, -1].argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08a0b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Token: Bras            | Logit: 43.0000\n",
      ">>> Token: Brasil          | Logit: 42.5000\n",
      ">>> Token: Rio             | Logit: 41.5000\n",
      ">>> Token: _               | Logit: 40.0000\n",
      ">>> Token: a               | Logit: 39.7500\n",
      ">>> Token: not             | Logit: 39.5000\n",
      ">>> Token: São             | Logit: 39.2500\n",
      ">>> Token: ...             | Logit: 39.0000\n",
      ">>> Token: located         | Logit: 39.0000\n",
      ">>> Token: the             | Logit: 39.0000\n"
     ]
    }
   ],
   "source": [
    "# Decode the topk tokens ang get thei logit values\n",
    "topk: int = 10\n",
    "topk_logits, topk_indices = torch.topk(lm_head_output[0, -1], k=topk)\n",
    "for logit, index in zip(topk_logits, topk_indices):\n",
    "    token = tokenizer.decode(index)\n",
    "    print(f\">>> Token: {token:15} | Logit: {logit.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78adfca1",
   "metadata": {},
   "source": [
    "### Parallel Token Processing and Context Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc7ba7",
   "metadata": {},
   "source": [
    "### Speeding Up Generation by Caching Keys and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82fedebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14326ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.39 s ± 77.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=20,\n",
    "  use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "603b6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.7 s ± 1.01 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=20,\n",
    "  use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486afb1",
   "metadata": {},
   "source": [
    "### Inside the Transformer Block"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
