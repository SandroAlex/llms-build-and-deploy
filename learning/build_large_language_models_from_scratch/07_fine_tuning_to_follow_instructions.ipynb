{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6903f6e",
   "metadata": {},
   "source": [
    "# Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90f7d4",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70c24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1174a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c15c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 18:30:08.227474: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-10 18:30:08.231281: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-10 18:30:08.242592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765391408.261859    6850 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765391408.267640    6850 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765391408.282768    6850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765391408.282790    6850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765391408.282792    6850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765391408.282794    6850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-10 18:30:08.288126: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Initial imports.\n",
    "import sys\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from functools import partial\n",
    "from importlib.metadata import version\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62be64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillar_path: str = \"/llm_app/learning/build_large_language_models_from_scratch/\"\n",
    "\n",
    "if ancillar_path not in sys.path:\n",
    "    sys.path.append(ancillar_path)\n",
    "\n",
    "import ancillar as aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c710064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.3\n",
      "matplotlib version: 3.10.0\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1+cpu\n",
      "tqdm version: 4.67.1\n",
      "tensorflow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "pkgs: List[str] = [\n",
    "    \"numpy\",  # PyTorch & TensorFlow dependency\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",  # Tokenizer\n",
    "    \"torch\",  # Deep learning library\n",
    "    \"tqdm\",  # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f4658",
   "metadata": {},
   "source": [
    "## Preparing a Dataset for Supervised Instruction Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07fd30ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = aux.download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8d0d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf930e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89aa3bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = aux.format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cacfc996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Random example entry (index 821):\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Arrange these historical periods in chronological order: Renaissance, Roman Empire, Victorian Era.\n",
      "\n",
      "### Response:\n",
      "1. Roman Empire\n",
      "2. Renaissance\n",
      "3. Victorian Era\n"
     ]
    }
   ],
   "source": [
    "idx: int = np.random.randint(0, len(data))\n",
    "print(f\">>> Random example entry (index {idx}):\\n\")\n",
    "\n",
    "model_input = aux.format_input(data[idx])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[idx]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10dcecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)  # Use 85% of the data for training.\n",
    "test_portion = int(len(data) * 0.1)  # Use 10% for testing.\n",
    "val_portion = (\n",
    "    len(data) - train_portion - test_portion\n",
    ")  # Use remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion : train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion :]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354cf3e",
   "metadata": {},
   "source": [
    "## Organizing Data into Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee979d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb15c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c23ceb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "print(aux.custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4434cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = aux.custom_collate_draft_2(batch)\n",
    "\n",
    "print(inputs)\n",
    "print()\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81a8b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = aux.custom_collate_fn(batch)\n",
    "\n",
    "print(inputs)\n",
    "print()\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0dbb087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor([\n",
    "    [-1.0, 1.0],\n",
    "    [-0.5, 1.5]\n",
    "])\n",
    "\n",
    "# Correct token indices to generate.\n",
    "targets_1 = torch.tensor([0, 1]) \n",
    "\n",
    "# Compute the loss.\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aa870ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor([\n",
    "    [-1.0, 1.0],\n",
    "    [-0.5, 1.5],\n",
    "    [-0.5, 1.5]\n",
    "])\n",
    "\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71a9838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4d6fa",
   "metadata": {},
   "source": [
    "## Creating Data Loaders for an Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f47ffd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU, MPS, or CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# Use MPS (Apple Silicon) if GPU is not available\n",
    "elif torch.backends.mps.is_available():\n",
    "    \n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0b43e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show it\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39d04a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<function custom_collate_fn at 0x7f1dccebe8c0>, device=device(type='cpu'), allowed_max_length=1024)\n"
     ]
    }
   ],
   "source": [
    "customized_collate_fn = partial(\n",
    "    aux.custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")\n",
    "\n",
    "print(customized_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34be4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try to increase this number if parallel Python processes are\n",
    "# supported by your operating system.\n",
    "num_workers = 0\n",
    "\n",
    "# Batch size for data loaders.\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5c0e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = aux.InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06c5196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = aux.InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78963cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = aux.InstructionDataset(test_data, tokenizer)\n",
    "test_loader = aux.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe0fe131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(9):\n",
    "\n",
    "    # Grab a single batch from the training data loader.\n",
    "    example_batch: Tuple[torch.Tensor, torch.Tensor] = next(iter(train_loader))\n",
    "\n",
    "    # Unpack the batch into inputs and targets.\n",
    "    inputs, targets = example_batch\n",
    "\n",
    "    # Show shapes\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49e0eb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "        21017, 46486,    25,   198,    35,  2357,  3810,  1771,   262,  1813,\n",
       "          734,  2456,   389,  6171, 43612,   393,   281,  1122,    88,   907,\n",
       "           13,   198,   198, 21017, 23412,    25,   198, 10434,   532, 13707,\n",
       "          198,   198, 21017, 18261,    25,   198, 10434,   290, 13707,   389,\n",
       "          281,  1122,    88,   907,    13, 50256, 50256, 50256, 50256, 50256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6590996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
       "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
       "        46486,    25,   198,    35,  2357,  3810,  1771,   262,  1813,   734,\n",
       "         2456,   389,  6171, 43612,   393,   281,  1122,    88,   907,    13,\n",
       "          198,   198, 21017, 23412,    25,   198, 10434,   532, 13707,   198,\n",
       "          198, 21017, 18261,    25,   198, 10434,   290, 13707,   389,   281,\n",
       "         1122,    88,   907,    13, 50256,  -100,  -100,  -100,  -100,  -100])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a277fd6",
   "metadata": {},
   "source": [
    "## Loading a Pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b1511a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size: str = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aac4504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2-from-scratch/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2-from-scratch/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2-from-scratch/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2-from-scratch/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2-from-scratch/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2-from-scratch/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2-from-scratch/355M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2-from-scratch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d296a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aux.GPTModel(BASE_CONFIG)\n",
    "aux.load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da78e27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1e571b8e90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3f9b292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "input_text: str = aux.format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e17ac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 928 ms, total: 1min 25s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "token_ids: torch.Tensor = aux.generate(\n",
    "    model=model,\n",
    "    idx=aux.text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=70,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "\n",
    "generated_text: str = aux.token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b57ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Number of input tokens: torch.Size([1, 41])\n",
      ">>> Number of generated tokens: torch.Size([1, 111])\n"
     ]
    }
   ],
   "source": [
    "print(f\">>> Number of input tokens: {aux.text_to_token_ids(input_text, tokenizer).shape}\")\n",
    "print(f\">>> Number of generated tokens: {token_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a56f1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: '\n"
     ]
    }
   ],
   "source": [
    "# The generate function returns the combined input and output text.\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d3fb18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: '\n"
     ]
    }
   ],
   "source": [
    "response_text: str = generated_text[len(input_text):].strip()\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317688d8",
   "metadata": {},
   "source": [
    "## Fine-tuning the LLM on Instruction Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
