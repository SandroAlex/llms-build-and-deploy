{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More or less two minutes to install these packages.\n",
    "# !pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 400 ms, total: 2.67 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load packages.\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import tiktoken\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillar_path = \"/llm_app/notebooks/build_large_language_models_from_scratch/\"\n",
    "if ancillar_path not in sys.path:\n",
    "    sys.path.append(ancillar_path)\n",
    "\n",
    "import ancillar as aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size.\n",
    "    \"context_length\": 256,  # Shortened context length (orig: 1024).\n",
    "    \"emb_dim\": 768,  # Embedding dimension.\n",
    "    \"n_heads\": 12,  # Number of attention heads.\n",
    "    \"n_layers\": 12,  # Number of layers.\n",
    "    \"drop_rate\": 0.1,  # Dropout rate.\n",
    "    \"qkv_bias\": False,  # Query-key-value bias.\n",
    "}\n",
    "\n",
    "torch.manual_seed(123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Generative Text Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPT to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aux.GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# Disable dropout during inference.\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.1: Utility functions for text to token ID conversion.\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "    # Add batch dimension.\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "\n",
    "    # Remove batch dimension.\n",
    "    flat = token_ids.squeeze(0)\n",
    "\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n",
      "CPU times: user 2.35 s, sys: 24 ms, total: 2.38 s\n",
      "Wall time: 873 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = aux.generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Text Generation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[16833, 3626, 6100], [40, 1107, 588]]  # [\"every effort moves\",\n",
    ")  #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor(\n",
    "    [[3626, 6100, 345], [1107, 588, 11311]]  # [\" effort moves you\",\n",
    ")  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Disables gradient tracking since we are not training yet.\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)  # Probability of each token in vocabulary\n",
    "print(probas.shape)  # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> `token_ids` shape:\n",
      "torch.Size([2, 3, 1])\n",
      "\n",
      ">>> Token IDs:\n",
      "tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "print(f\">>> `token_ids` shape:\\n{token_ids.shape}\\n\")\n",
    "print(f\">>> Token IDs:\\n{token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Logarithm of token probabilities:\n",
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities.\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "\n",
    "print(f\">>> Logarithm of token probabilities:\\n{log_probas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Average log probability:\n",
      "-10.79397201538086\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "\n",
    "print(f\">>> Average log probability:\\n{avg_log_probas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Negative average log probability:\n",
      "10.79397201538086\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "\n",
    "print(f\">>> Negative average log probability:\\n{neg_avg_log_probas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Logits shape: torch.Size([2, 3, 50257])\n",
      ">>> Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Logits shape:\", logits.shape)\n",
    "print(\">>> Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Flattened logits: torch.Size([6, 50257])\n",
      ">>> Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\">>> Flattened logits:\", logits_flat.shape)\n",
    "print(\">>> Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48726.1953)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Training and Validation Set Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = (\n",
    "    \"/llm_app/notebooks/build_large_language_models_from_scratch/the-verdict.txt\"\n",
    ")\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode(\"utf-8\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Length of training data: 18431\n",
      ">>> Length of validation data: 2048\n"
     ]
    }
   ],
   "source": [
    "# Train / validation ratio.\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "print(\">>> Length of training data:\", len(train_data))\n",
    "print(\">>> Length of validation data:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = aux.create_dataloader_v1(\n",
    "    txt=train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = aux.create_dataloader_v1(\n",
    "    txt=val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      ">>> Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\n>>> Validation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "\n",
    "    # The transfer to a given device allows us to transfer the data to a GPU.\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # Iteratives over all batches if no fixed num_batches is specified.\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "\n",
    "    # Reduce the number of batches to match the total number of batches in the data loader\n",
    "    # if num_batches exceeds the number of batches in the data loader.\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training loss: 10.987582206726074\n",
      ">>> Validation loss: 10.98110580444336\n",
      "CPU times: user 14.8 s, sys: 4.75 s, total: 19.5 s\n",
      "Wall time: 4.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\">>> Device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "# For reproducibility due to the shuffling in the data loader.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Disable gradient tracking for efficiency because we are not training, yet.\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\">>> Training loss:\", train_loss)\n",
    "print(\">>> Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    \"\"\"\n",
    "    Prints the training and validation set losses after each model update so we can evaluate whether the training improves the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropout is disabled during evaluation for stable, reproducible results.\n",
    "    model.eval()\n",
    "\n",
    "    # Disables gradient tracking, which is not required during evaluation, to reduce the computational overhead.\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    \"\"\"\n",
    "    Provides a concrete text example generated by the model to judge its capabilities during training.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        token_ids = aux.generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    # Compact print format.\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    eval_freq,\n",
    "    eval_iter,\n",
    "    start_context,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    # Listing 5.3: The main function for pretraining LLMs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to track losses and tokens seen.\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop.\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Set model to training mode.\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over all batches in the training data.\n",
    "        for input_batch, target_batch in train_loader:\n",
    "\n",
    "            # Reset loss gradients from previous batch iteration.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate loss.\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            #  Calculate loss gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model weights using loss gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the number of tokens seen and global step.\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step.\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(\n",
    "                    f\">>> Epoch: {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Validation loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # Print a sample text after each epoch.\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch: 1 (Step 000000): Train loss 9.783, Validation loss 9.927\n",
      ">>> Epoch: 1 (Step 000005): Train loss 7.985, Validation loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      ">>> Epoch: 2 (Step 000010): Train loss 6.753, Validation loss 7.048\n",
      ">>> Epoch: 2 (Step 000015): Train loss 6.114, Validation loss 6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      ">>> Epoch: 3 (Step 000020): Train loss 5.525, Validation loss 6.490\n",
      ">>> Epoch: 3 (Step 000025): Train loss 5.324, Validation loss 6.388\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      ">>> Epoch: 4 (Step 000030): Train loss 4.762, Validation loss 6.360\n",
      ">>> Epoch: 4 (Step 000035): Train loss 4.461, Validation loss 6.257\n",
      "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
      ">>> Epoch: 5 (Step 000040): Train loss 3.833, Validation loss 6.197\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      ">>> Epoch: 6 (Step 000045): Train loss 3.352, Validation loss 6.139\n",
      ">>> Epoch: 6 (Step 000050): Train loss 2.862, Validation loss 6.112\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.         \"Oh, I had a, and down, and he was his\n",
      ">>> Epoch: 7 (Step 000055): Train loss 2.348, Validation loss 6.138\n",
      ">>> Epoch: 7 (Step 000060): Train loss 2.085, Validation loss 6.179\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
      ">>> Epoch: 8 (Step 000065): Train loss 1.522, Validation loss 6.176\n",
      ">>> Epoch: 8 (Step 000070): Train loss 1.273, Validation loss 6.178\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me. \"Oh, and I was, one of Jack's degree to the display of the his head to look up at the honour being _mine_--because he didn't want\n",
      ">>> Epoch: 9 (Step 000075): Train loss 1.000, Validation loss 6.277\n",
      ">>> Epoch: 9 (Step 000080): Train loss 0.719, Validation loss 6.281\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      ">>> Epoch: 10 (Step 000085): Train loss 0.506, Validation loss 6.325\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n",
      ">>> Training completed in 3.16 minutes.\n",
      "CPU times: user 11min 4s, sys: 1min 16s, total: 12min 20s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = aux.GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "# The .parameters() method returns all trainable weight parameters of the model.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\">>> Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    # Only show integer labels on x-axis.\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Create a second x-axis for tokens seen.\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis.\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks.\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    # Adjust layout to make room.\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies to Control Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = aux.generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    6.0907,     0.1631,     0.0100,    57.2120,     0.3419,     0.0133,\n",
       "            0.0101,    35.7576,     0.4012])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "\n",
    "    torch.manual_seed(123)  # Manual seed for reproducibility.\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature values.\n",
    "# Original, higher confidence, and lower confidence.\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "# Calculate scaled probabilities.\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGGCAYAAADmRxfNAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAY7hJREFUeJzt3XlYVGX/P/D3sA0gMIiyKgoqKRrimuK+kLg8pmGaZk8uaFmuIJqWIOCCUan55J77krao5ZIbCbjgvqeiKAgloKkwIjIMzPn94df5OYLIMMA5yPt1XVxx7nPPPZ854Ol8uDeZIAgCiIiIiIiIDGAkdgBERERERFT5MbEgIiIiIiKDMbEgIiIiIiKDMbEgIiIiIiKDMbEgIiIiIiKDMbEgIiIiIiKDMbEgIiIiIiKDMbEgIiIiIiKDmYgdgBRoNBrcuXMH1tbWkMlkYodDRERERCQJgiDg0aNHcHFxgZFR8X0STCwA3LlzB66urmKHQUREREQkSampqahdu3axdZhYALC2tgbw9ILZ2NiIHA0RERERkTQolUq4urpqn5eLw8QC0A5/srGxYWJBRERERPSCkkwX4ORtIiIiIiIyGBMLIiIiIiIyGBMLIiIiIiIyGOdYEBEREZWSRqNBXl6e2GEQlZqpqSmMjY3LpC1RE4u4uDh8/fXXOHPmDNLS0rB9+3b0799fe14QBMycORMrV65EZmYm2rdvj6VLl8LDw0Nb58GDBxg/fjx27twJIyMjDBgwAN999x2srKxE+ERERERUVeTl5SEpKQkajUbsUIgMYmtrCycnJ4P3cxM1sXj8+DG8vb0xcuRI+Pv7FzofFRWFRYsWYd26dXB3d0dISAj8/Pxw5coVmJubAwCGDh2KtLQ0HDhwAGq1GiNGjMDHH3+MzZs3V/THISIioipCEASkpaXB2NgYrq6ur9w4jEiKBEFATk4O7t69CwBwdnY2qD2ZIAhCWQRmKJlMptNjIQgCXFxcMHnyZAQHBwMAsrKy4OjoiLVr12Lw4MG4evUqGjdujFOnTqFVq1YAgL1796J37974+++/4eLiUqL3ViqVUCgUyMrK4nKzRERE9EpqtRqJiYlwcXGBQqEQOxwig9y/fx93797FG2+8UWhYlD7PyZJNr5OSkpCeng5fX19tmUKhQJs2bRAfHw8AiI+Ph62trTapAABfX18YGRnhxIkTFR4zERERVQ0FBQUAADMzM5EjITKcpaUlgKcJsyEkO3k7PT0dAODo6KhT7ujoqD2Xnp4OBwcHnfMmJiaws7PT1imKSqWCSqXSHiuVSgBPL6ahF5SIiIhef2q1GoIgQBAEzrGgSu/Z77JarS7UY6HPs7FkE4vyFBkZifDw8ELl+/fv12ZsRERERC9jYmICJycnZGdnc1UoqvTy8vLw5MkTxMXFIT8/X+dcTk5OiduRbGLh5OQEAMjIyNCZSJKRkYFmzZpp6zybbPJMfn4+Hjx4oH19UaZPn46goCDtsVKphKurK3r06ME5FkRERPRKubm5SE1NhZWVlXZBGaLKKjc3FxYWFujUqVOh3+dnI3tKQrKJhbu7O5ycnBAdHa1NJJRKJU6cOIFPP/0UAODj44PMzEycOXMGLVu2BAD8+eef0Gg0aNOmzUvblsvlkMvlhcpNTU1hampa9h+GiIiIXisFBQWQyWQwMjLSWRHKbdruCo0jeV6fEtd91VKiM2fORFhYmIERSYubmxsmTZqESZMmiR1KqU2YMAFHjx7F5cuX4enpifPnz5f5exgZGUEmkxX5LKzPs7GoiUV2djYSExO1x0lJSTh//jzs7OxQp04dTJo0CbNnz4aHh4d2uVkXFxftylGenp7o2bMnRo8ejWXLlkGtVmPcuHEYPHhwiVeEIiIqM2F6rgwTllU+cRARFSEtLU37/datWxEaGoqEhARtWWXZA0wQBBQUFMDEpOIeY/Py8kSdqD9y5EicOHECFy9eFC2GkhB1VajTp0+jefPmaN68OQAgKCgIzZs3R2hoKABg6tSpGD9+PD7++GO0bt0a2dnZ2Lt3r04XzaZNm9CoUSN0794dvXv3RocOHbBixQpRPg8RERGRVDk5OWm/FAoFZDKZTtmWLVvg6ekJc3NzNGrUCEuWLNG+Njk5GTKZDD/99BM6duwICwsLtG7dGtevX9cu+29lZYVevXrh3r172tcNHz4c/fv3R3h4OOzt7WFjY4MxY8bozEvRaDSIjIyEu7s7LCws4O3tjV9++UV7PiYmBjKZDH/88QdatmwJuVyOI0eO4ObNm+jXrx8cHR1hZWWF1q1b4+DBg9rXdenSBbdv30ZgYCBkMpm2xyYsLEw7GuaZhQsXws3NrVDcc+bMgYuLCxo2bAgASE1NxaBBg2Braws7Ozv069cPycnJZfHjealFixZh7NixqFevXrm+T1kQtceiS5cuKG4bDZlMhoiICERERLy0jp2dHTfDIyIiIjLApk2bEBoaiu+//x7NmzfHuXPnMHr0aFSrVg3Dhg3T1ps5cyYWLlyIOnXqYOTIkfjggw9gbW2N7777DpaWlhg0aBBCQ0OxdOlS7Wuio6Nhbm6OmJgYJCcnY8SIEahRowbmzJkD4OmiOhs3bsSyZcvg4eGBuLg4fPjhh7C3t0fnzp217UybNg3ffPMN6tWrh+rVqyM1NRW9e/fGnDlzIJfLsX79evTt2xcJCQmoU6cOtm3bBm9vb3z88ccYPXq03tckOjoaNjY2OHDgAICnqyP5+fnBx8cHhw8fhomJCWbPno2ePXvi4sWLL+3ReFVP0Icffohly5bpHZ8USXaOBRERERFVjJkzZ+Lbb7+Fv78/gKdzXa9cuYLly5frJBbBwcHw8/MDAEycOBFDhgxBdHQ02rdvDwAICAjA2rVrddo2MzPD6tWrYWlpiSZNmiAiIgJTpkzBrFmzoFarMXfuXBw8eBA+Pj4AgHr16uHIkSNYvny5TmIRERGBt99+W3tsZ2cHb29v7fGsWbOwfft2/P777xg3bhzs7OxgbGwMa2vrYhf1eZlq1arhhx9+0CYMGzduhEajwQ8//KDt/VizZg1sbW0RExODHj16FNnOq+ZEvE4LBzGxICIiIqrCHj9+jJs3byIgIEDnL/v5+fmFdhVv2rSp9vtne415eXnplL24Yqe3t7fOcv4+Pj7Izs5GamoqsrOzkZOTo5MwAE/nNDwbKv/M8xsiA0/n6oaFhWH37t1IS0tDfn4+njx5gpSUFH0+/kt5eXnp9EJcuHABiYmJsLa21qmXm5uLmzdvvrSdBg0alEk8lQETCyIiIqIqLDs7GwCwcuXKQqtqvrhZ2vMrBD37q/2LZfpsGPjsvXfv3o1atWrpnHtxBc9q1arpHAcHB+PAgQP45ptv0KBBA1hYWOC999575b4iRkZGhYbiF7UJ3Ivvl52djZYtW2LTpk2F6trb27/0/TgUioiIiIiqBEdHR7i4uODWrVsYOnRombd/4cIFPHnyBBYWFgCA48ePw8rKCq6urrCzs4NcLkdKSorOsKeSOHr0KIYPH453330XwNMH/xcnUpuZmaGgoECnzN7eHunp6RAEQZsclWQJ1xYtWmDr1q1wcHDQa/gSh0IRERERUZURHh6OCRMmQKFQoGfPnlCpVDh9+jQePnyos6lwaeTl5SEgIAAzZsxAcnIyZs6ciXHjxsHIyAjW1tYIDg5GYGAgNBoNOnTogKysLBw9ehQ2NjY68zte5OHhgW3btqFv376QyWQICQkp1Fvi5uaGuLg4DB48GHK5HDVr1kSXLl1w7949REVF4b333sPevXvxxx9/vPIBf+jQofj666/Rr18/REREoHbt2rh9+za2bduGqVOnonbt2kW+ztChUImJicjOzkZ6ejqePHmiTVQaN24s6hK4RRF1uVkiIiIiEt+oUaPwww8/YM2aNfDy8kLnzp2xdu1auLu7G9x29+7d4eHhgU6dOuH999/HO++8o7MR36xZsxASEoLIyEjtHmW7d+9+5XvPnz8f1atXR7t27dC3b1/4+fmhRYsWOnUiIiKQnJyM+vXra4creXp6YsmSJVi8eDG8vb1x8uRJBAcHv/JzWFpaIi4uDnXq1IG/vz88PT0REBCA3Nzccu11GDVqFJo3b47ly5fj+vXr2q0a7ty5U27vWVoyobj1XqsIpVIJhUKBrKys16o7iogqGDfII6oycnNzkZSUBHd3d539tUjX8OHDkZmZiR07dogdChWjuN9nfZ6T2WNBREREREQGY2JBREREREQG4+RtIiIiIioXL26WR6839lgQEREREZHBmFgQEREREZHBmFgQEREREZHBmFgQEREREZHBmFgQEREREZHBmFgQEREREZHBmFgQERERVQEymazYr7CwMLFDLHNubm5YuHCh2GEYJCUlBX369IGlpSUcHBwwZcoU5OfnF/uaOXPmoF27drC0tIStrW3FBAruY0FERERUdsIUFfx+WSWumpaWpv1+69atCA0NRUJCgrbMysqqTEMrL4IgoKCgACYmFfcYm5eXBzMzswp7v2cKCgrQp08fODk54dixY0hLS8NHH30EU1NTzJ0796Wvy8vLw8CBA+Hj44NVq1ZVWLzssSAiIiKqApycnLRfCoUCMplMp2zLli3w9PSEubk5GjVqhCVLlmhfm5ycDJlMhp9++gkdO3aEhYUFWrdujevXr+PUqVNo1aoVrKys0KtXL9y7d0/7uuHDh6N///4IDw+Hvb09bGxsMGbMGOTl5WnraDQaREZGwt3dHRYWFvD29sYvv/yiPR8TEwOZTIY//vgDLVu2hFwux5EjR3Dz5k3069cPjo6OsLKyQuvWrXHw4EHt67p06YLbt28jMDBQ2ysDAGFhYWjWrJnOtVm4cCHc3NwKxT1nzhy4uLigYcOGAIDU1FQMGjQItra2sLOzQ79+/ZCcnFwWP54i7d+/H1euXMHGjRvRrFkz9OrVC7NmzcLixYt1ruGLwsPDERgYCC8vr3KLrSiSTywePXqESZMmoW7durCwsEC7du1w6tQp7XlBEBAaGgpnZ2dYWFjA19cXN27cEDFiIiIiospl06ZNCA0NxZw5c3D16lXMnTsXISEhWLdunU69mTNnYsaMGTh79ixMTEzwwQcfYOrUqfjuu+9w+PBhJCYmIjQ0VOc10dHRuHr1KmJiYvDjjz9i27ZtCA8P156PjIzE+vXrsWzZMvz1118IDAzEhx9+iNjYWJ12pk2bhnnz5uHq1ato2rQpsrOz0bt3b0RHR+PcuXPo2bMn+vbti5SUFADAtm3bULt2bURERCAtLU2nx6YkoqOjkZCQgAMHDmDXrl1Qq9Xw8/ODtbU1Dh8+jKNHj8LKygo9e/Ys9iHfysqq2K8xY8a89LXx8fHw8vKCo6OjtszPzw9KpRJ//fWXXp+nIkh+KNSoUaNw+fJlbNiwAS4uLti4cSN8fX1x5coV1KpVC1FRUVi0aBHWrVsHd3d3hISEwM/PD1euXIG5ubnY4RMRERFJ3syZM/Htt9/C398fAODu7o4rV65g+fLlGDZsmLZecHAw/Pz8AAATJ07EkCFDEB0djfbt2wMAAgICsHbtWp22zczMsHr1alhaWqJJkyaIiIjAlClTMGvWLKjVasydOxcHDx6Ej48PAKBevXo4cuQIli9fjs6dO2vbiYiIwNtvv609trOzg7e3t/Z41qxZ2L59O37//XeMGzcOdnZ2MDY2hrW1NZycnPS+JtWqVcMPP/ygHQK1ceNGaDQa/PDDD9rejzVr1sDW1hYxMTHo0aNHke2cP3++2PexsbF56bn09HSdpAKA9jg9Pb2kH6XCSDqxePLkCX799Vf89ttv6NSpE4Cn3Vc7d+7E0qVLMWvWLCxcuBAzZsxAv379AADr16+Ho6MjduzYgcGDB4sZPhEREZHkPX78GDdv3kRAQABGjx6tLc/Pz4dCoTtnpGnTptrvnz3gPj/cxtHREXfv3tV5jbe3NywtLbXHPj4+yM7ORmpqKrKzs5GTk6OTMABP5wg0b95cp6xVq1Y6x9nZ2QgLC8Pu3buRlpaG/Px8PHnyRNtjYSgvLy+deRUXLlxAYmIirK2tderl5ubi5s2bL22nQYMGZRJPZSDpxCI/Px8FBQWFeh4sLCxw5MgRJCUlIT09Hb6+vtpzCoUCbdq0QXx8/EsTC5VKBZVKpT1WKpUAALVaDbVaXQ6fhIiqBCM9e0l5vyGqtNRqNQRBgEajgUaj0ZZX9Bjz59+7NK/TaDTa56Dly5ejTZs2OvWMjY11PuOzY+DpcPSiyp6v/6zO83EW9d47d+5ErVq1dN5bLpfrtGVhYaHTzuTJk3Hw4EFERUWhQYMGsLCwwKBBg6BSqXTqPYvpGZlMVqjs2VCm5+O2tLTUqfPo0SO0bNkSGzZsKHQ97e3tX/qzKK5HAgCGDh2KpUuXFnnO0dERJ0+e1Gn72ZAuBweHV/78n7/Wr6onCALUajWMjY11zunzbCzpxMLa2ho+Pj6YNWsWPD094ejoiB9//BHx8fFo0KCBtguoqC6i4rqHIiMjdcb2PbN//36djJqISC/eK/Srv2dP+cRBROXOxMQETk5OyM7O1hlfb1vBcTx7MNdXbm4uBEGAUqmEhYUFnJ2dce3aNfTt27fI98jOzgbwtHfj2Xvm5OQAePrAbWRkVKhd4OlD6fnz55GRkQELCwsATydjW1lZQaFQwMTEBHK5HAkJCYV6KJ69d1HvAwCHDx/G4MGD0b17dwBPezCSkpLg4+OjfX8TExOdmIGncx7S0tKQlZWlHdJ06tQpnURHrVYjPz9f53Wenp7YunUrzM3Ni0wWXvaziIuLK7L8GWtr65e+tmnTppg7dy5u3rwJe3t7AE+TMGtra9SuXfuVP/8Xfx4vk5eXhydPniAuLq7QUrbPrn9JSDqxAIANGzZg5MiRqFWrFoyNjdGiRQsMGTIEZ86cKXWb06dPR1BQkPZYqVTC1dUVPXr0eGVWSUT0UpG19as//e/yiYOIyl1ubi5SU1NhZWUl6pzO0j63mJubQyaTaV8fFhaGSZMmwcHBAX5+flCpVDh9+jQyMzMRGBioXYq2WrVq2tc8+2OstbW1tuzFdk1NTaFWqxEUFIQvv/wSycnJ+OqrrzB27FjY2trC1tYWkydPxowZMyCXy9GhQwdkZWXh2LFjsLa2xrBhw4p8HwBo2LAh9uzZgwEDBkAmkyE0NBSCIMDMzExbz93dHSdPnsSjR48gl8tRs2ZN9OzZE1OmTMHy5csxYMAA7Nu3D9HR0bCxsdGJ28TEROf9AgICsHjxYgwbNgxhYWGoXbs2bt++je3bt2PKlCmoXbvo/we8uAKVPvr374/GjRtj7Nix+Oqrr5Ceno65c+di7Nix2kTj5MmTGD58OA4cOKDt9UlJScGDBw9w7949aDQa3Lp1C8DTYVlFLSucm5sLCwsLdOrUqdDvsz7Jq+QTi/r16yM2NlabbTo7O+P9999HvXr1tBNxMjIy4OzsrH1NRkZGsT9EuVwOuVxeqNzU1BSmpqZl/hmIqIrQ5OpXn/cbokqroKAAMpkMRkZGOn9Fr2ilfe9nr3v2348//hhWVlb4+uuvMXXqVFSrVg1eXl6YNGmSzmd88fviyoCnw466d++ON954A126dIFKpcKQIUMQHh6urTN79mw4ODjgq6++wieffAJbW1u0aNECX3zxxUvfGwAWLFiAkSNHokOHDqhZsyY+//xzPHr0SPtzAZ5O6P7kk0/g4eEBlUoFQRDQpEkTLFmyBHPnzsXs2bMxYMAABAcHY8WKFTpxP98O8LSnIy4uDp9//jnee+89PHr0CLVq1UL37t1ha2tbLr8HRkZG2LVrFz799FO0b98e1apVw7BhwzBr1iydXqKEhAQUFBRoy8LCwnRW9GrZsiUA4NChQ+jSpUuR7yOTyYp8Ftbn2VgmPBv8Vkk8fPgQ7u7uiIqKwujRo+Hi4oLg4GBMnjwZwNOsysHBAWvXri3x5G2lUgmFQoGsrCz2WBBR6em7MZYeG1sRkbTk5uYiKSkJ7u7uXIWyGMOHD0dmZiZ27NghdihUjOJ+n/V5TpZ8j8W+ffsgCAIaNmyIxMRETJkyBY0aNcKIESMgk8kwadIkzJ49Gx4eHtrlZl1cXNC/f3+xQyciIiIiqjIkn1hkZWVh+vTp+Pvvv2FnZ4cBAwZgzpw52m6ZqVOn4vHjx/j444+RmZmJDh06YO/evfzrARERERFRBap0Q6HKA4dCEVGZ4FAooiqDQ6HodVJWQ6HEm21ERERERESvDSYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERFWATCYr9issLEzsEMucm5sbFi5cKHYYBinqZ7VlyxaxwyqS5HfeJiIiIqosvNZ5Vej7XRp2qcR109LStN9v3boVoaGhSEhI0JZZWVmVaWzlRRAEFBQUwMSk4h5j8/LyYGZmVmHv96I1a9agZ8+e2mNbW1vRYikOeyyIiIiIqgAnJyftl0KhgEwm0ynbsmULPD09YW5ujkaNGmHJkiXa1yYnJ0Mmk+Gnn35Cx44dYWFhgdatW+P69es4deoUWrVqBSsrK/Tq1Qv37t3Tvm748OHo378/wsPDYW9vDxsbG4wZMwZ5eXnaOhqNBpGRkXB3d4eFhQW8vb3xyy+/aM/HxMRAJpPhjz/+QMuWLSGXy3HkyBHcvHkT/fr1g6OjI6ysrNC6dWscPHhQ+7ouXbrg9u3bCAwM1P6lHwDCwsLQrFkznWuzcOFCuLm5FYp7zpw5cHFxQcOGDQEAqampGDRoEGxtbWFnZ4d+/fohOTm5LH48xbK1tdX5WUl1t3cmFkRERERV3KZNmxAaGoo5c+bg6tWrmDt3LkJCQrBu3TqdejNnzsSMGTNw9uxZmJiY4IMPPsDUqVPx3Xff4fDhw0hMTERoaKjOa6Kjo3H16lXExMTgxx9/xLZt2xAeHq49HxkZifXr12PZsmX466+/EBgYiA8//BCxsbE67UybNg3z5s3D1atX0bRpU2RnZ6N3796Ijo7GuXPn0LNnT/Tt2xcpKSkAgG3btqF27dqIiIhAWlqaTo9NSURHRyMhIQEHDhzArl27oFar4efnB2traxw+fBhHjx6FlZUVevbsqZMovcjKyqrYrzFjxrwylrFjx6JmzZp46623sHr1agiCoNdnqSgcCkVERERUxc2cORPffvst/P39AQDu7u64cuUKli9fjmHDhmnrBQcHw8/PDwAwceJEDBkyBNHR0Wjfvj0AICAgAGvXrtVp28zMDKtXr4alpSWaNGmCiIgITJkyBbNmzYJarcbcuXNx8OBB+Pj4AADq1auHI0eOYPny5ejcubO2nYiICLz99tvaYzs7O3h7e2uPZ82ahe3bt+P333/HuHHjYGdnB2NjY1hbW8PJyUnva1KtWjX88MMP2iFQGzduhEajwQ8//KDt/VizZg1sbW0RExODHj16FNnO+fPni30fGxubYs9HRESgW7dusLS0xP79+/HZZ58hOzsbEyZM0PszlTcmFkRERERV2OPHj3Hz5k0EBARg9OjR2vL8/HwoFAqduk2bNtV+7+joCADw8vLSKbt7967Oa7y9vWFpaak99vHxQXZ2NlJTU5GdnY2cnBydhAF4OqehefPmOmWtWrXSOc7OzkZYWBh2796NtLQ05Ofn48mTJ9oeC0N5eXnpzKu4cOECEhMTYW1trVMvNzcXN2/efGk7DRo0MCiOkJAQ7ffNmzfH48eP8fXXXzOxICIiIiJpyc7OBgCsXLkSbdq00TlnbGysc2xqaqr9/tlf7V8s02g0er/37t27UatWLZ1zcrlc57hatWo6x8HBwThw4AC++eYbNGjQABYWFnjvvfeKHZYEAEZGRoWGEqnV6kL1Xny/7OxstGzZEps2bSpU197e/qXv96pJ8R9++CGWLVtWbJ3ntWnTBrNmzYJKpSp0jcTGxIKIiIioCnN0dISLiwtu3bqFoUOHlnn7Fy5cwJMnT2BhYQEAOH78OKysrODq6go7OzvI5XKkpKToDHsqiaNHj2L48OF49913ATx98H9xIrWZmRkKCgp0yuzt7ZGeng5BELTJ0auGKwFAixYtsHXrVjg4OLxy+NLzDB0KVVR71atXl1xSATCxICIiIqrywsPDMWHCBCgUCvTs2RMqlQqnT5/Gw4cPERQUZFDbeXl5CAgIwIwZM5CcnIyZM2di3LhxMDIygrW1NYKDgxEYGAiNRoMOHTogKysLR48ehY2Njc78jhd5eHhg27Zt6Nu3L2QyGUJCQgr1lri5uSEuLg6DBw+GXC5HzZo10aVLF9y7dw9RUVF47733sHfvXvzxxx+vfMAfOnQovv76a/Tr1w8RERGoXbs2bt++jW3btmHq1KmoXbt2ka8zZCjUzp07kZGRgbZt28Lc3BwHDhzA3LlzERwcXOo2yxNXhSIiIiKq4kaNGoUffvgBa9asgZeXFzp37oy1a9fC3d3d4La7d+8ODw8PdOrUCe+//z7eeecdnc34Zs2ahZCQEERGRsLT0xM9e/bE7t27X/ne8+fPR/Xq1dGuXTv07dsXfn5+aNGihU6diIgIJCcno379+trhSp6enliyZAkWL14Mb29vnDx5skQP6paWloiLi0OdOnXg7+8PT09PBAQEIDc3V+9eh5IyNTXF4sWL4ePjg2bNmmH58uWYP38+Zs6cWS7vZyiZINX1qiqQUqmEQqFAVlZWuf1iEFEVEKZ4dR2d+lnlEwcRlbvc3FwkJSXB3d1dsnsKSMHw4cORmZmJHTt2iB0KFaO432d9npMl3WNRUFCAkJAQ7YYp9evXx6xZs3Qm3AiCgNDQUDg7O8PCwgK+vr64ceOGiFETEREREVU9kk4svvrqKyxduhTff/89rl69iq+++gpRUVH43//+p60TFRWFRYsWYdmyZThx4gSqVasGPz8/5Obmihg5EREREVHVIunJ28eOHUO/fv3Qp08fAE8n4Pz44484efIkgKe9FQsXLsSMGTPQr18/AMD69evh6OiIHTt2YPDgwaLFTkRERFTVvbhZHr3eJN1j0a5dO0RHR+P69esAni5XduTIEfTq1QsAkJSUhPT0dPj6+mpfo1Ao0KZNG8THx4sSMxERERFRVSTpHotp06ZBqVSiUaNGMDY2RkFBAebMmaNdYzk9PR3A/9/58RlHR0ftuaKoVCqoVCrtsVKpBPB0c5SiNkghIioRIz0ncPJ+Q1RpqdVqCIIAjUaj14ZwRFKk0WggCALUanWhTRH1eTaWdGLx008/YdOmTdi8eTOaNGmC8+fPY9KkSXBxcSl2XeNXiYyMRHh4eKHy/fv362w5T0SkF+8V+tXfs6d84iCicmdiYgInJydkZ2e/cqdnIqlTqVR48uQJ4uLikJ+fr3MuJyenxO1IerlZV1dXTJs2DWPHjtWWzZ49Gxs3bsS1a9dw69Yt1K9fH+fOnUOzZs20dTp37oxmzZrhu+++K7LdonosXF1d8e+//3K5WSIqvciiN0d6qel/l08cRFTu8vPzkZSUBBcXFz47UKV3//593Lt3D/Xq1SvUY6FUKlGzZs0SLTcr6R6LnJwcGBnpTgMxNjbWdjm6u7vDyckJ0dHR2sRCqVTixIkT+PTTT1/arlwuL3IbdFNTU5iampbdByCiqkWj52p0vN8QVVomJiaoVq0a/v33X5iZmRV6XiGqDARBQE5ODv79919Ur169yD1Z9Hk2lnRi0bdvX8yZMwd16tRBkyZNcO7cOcyfPx8jR44EAMhkMkyaNAmzZ8+Gh4cH3N3dERISAhcXF/Tv31/c4ImIiOi1JZPJ4OzsjKSkJNy+fVvscIgMYmtrCycnJ4PbkXRi8b///Q8hISH47LPPcPfuXbi4uOCTTz5BaGiots7UqVPx+PFjfPzxx8jMzESHDh2wd+9e7oJJRERE5crMzAweHh6cY0GVmqmpaaHhT6Ul6TkWFUWfrcqJiF4qTKFn/azyiYOIiKiM6POczAGBRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMCYWRERERERkMMknFm5ubpDJZIW+xo4dCwDIzc3F2LFjUaNGDVhZWWHAgAHIyMgQOWoiIiIioqpF8onFqVOnkJaWpv06cOAAAGDgwIEAgMDAQOzcuRM///wzYmNjcefOHfj7+4sZMhERERFRlWMidgCvYm9vr3M8b9481K9fH507d0ZWVhZWrVqFzZs3o1u3bgCANWvWwNPTE8ePH0fbtm3FCJmIiIiIqMqRfGLxvLy8PGzcuBFBQUGQyWQ4c+YM1Go1fH19tXUaNWqEOnXqID4+/qWJhUqlgkql0h4rlUoAgFqthlqtLt8PQUSvLyNz/erzfkNERBKnz7NxpUosduzYgczMTAwfPhwAkJ6eDjMzM9ja2urUc3R0RHp6+kvbiYyMRHh4eKHy/fv3w9LSsixDJqKqxHuFfvX37CmfOIiIiMpITk5OietWqsRi1apV6NWrF1xcXAxqZ/r06QgKCtIeK5VKuLq6okePHrCxsTE0TCKqqiJr61d/+t/lEwcREVEZeTaypyQqTWJx+/ZtHDx4ENu2bdOWOTk5IS8vD5mZmTq9FhkZGXBycnppW3K5HHK5vFC5qakpTE1NyzRuIqpCNLn61ef9hoiIJE6fZ2PJrwr1zJo1a+Dg4IA+ffpoy1q2bAlTU1NER0dryxISEpCSkgIfHx8xwiQiIiIiqpIqRY+FRqPBmjVrMGzYMJiY/P+QFQoFAgICEBQUBDs7O9jY2GD8+PHw8fHhilBERERERBWoUiQWBw8eREpKCkaOHFno3IIFC2BkZIQBAwZApVLBz88PS5YsESFKIiIiIqKqSyYIgiB2EGJTKpVQKBTIysri5G0iKr0whZ71s8onDiIiojKiz3NypZljQURERERE0sXEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDFaqxOLQoUNlHQcREREREVVipUosevbsifr162P27NlITU0t65iIiIiIiKiSKVVi8c8//2DcuHH45ZdfUK9ePfj5+eGnn35CXl5eWcdHRERERESVQKkSi5o1ayIwMBDnz5/HiRMn8MYbb+Czzz6Di4sLJkyYgAsXLpR1nEREREREJGEGT95u0aIFpk+fjnHjxiE7OxurV69Gy5Yt0bFjR/z1119lESMREREREUlcqRMLtVqNX375Bb1790bdunWxb98+fP/998jIyEBiYiLq1q2LgQMHlmWsREREREQkUaVKLMaPHw9nZ2d88skneOONN3Du3DnEx8dj1KhRqFatGtzc3PDNN9/g2rVrBgf4zz//4MMPP0SNGjVgYWEBLy8vnD59WnteEASEhobC2dkZFhYW8PX1xY0bNwx+XyIiIiIiKjmT0rzoypUr+N///gd/f3/I5fIi69SsWdPgZWkfPnyI9u3bo2vXrvjjjz9gb2+PGzduoHr16to6UVFRWLRoEdatWwd3d3eEhITAz88PV65cgbm5uUHvT0REREREJSMTBEHQ90VxcXFo164dTEx085L8/HwcO3YMnTp1KpPgpk2bhqNHj+Lw4cNFnhcEAS4uLpg8eTKCg4MBAFlZWXB0dMTatWsxePDgEr2PUqmEQqFAVlYWbGxsyiR2IqqCwhR61s8qnziIiIjKiD7PyaUaCtW1a1c8ePCgUHlWVha6du1amiaL9Pvvv6NVq1YYOHAgHBwc0Lx5c6xcuVJ7PikpCenp6fD19dWWKRQKtGnTBvHx8WUWBxERERERFa9UQ6EEQYBMJitUfv/+fVSrVs3goJ65desWli5diqCgIHzxxRc4deoUJkyYADMzMwwbNgzp6ekAAEdHR53XOTo6as8VRaVSQaVSaY+VSiWApxPS1Wp1mcVPRFWMkZ7DL3m/ISIiidPn2VivxMLf3x8AIJPJMHz4cJ35FQUFBbh48SLatWunT5PF0mg0aNWqFebOnQsAaN68OS5fvoxly5Zh2LBhpW43MjIS4eHhhcr3798PS0vLUrdLRFWc9wr96u/ZUz5xEBERlZGcnJwS19UrsVAono4fFgQB1tbWsLCw0J4zMzND27ZtMXr0aH2aLJazszMaN26sU+bp6Ylff/0VAODk5AQAyMjIgLOzs7ZORkYGmjVr9tJ2p0+fjqCgIO2xUqmEq6srevTowTkWRFR6kbX1qz/97/KJg4iIqIw8G9lTEnolFmvWrAEAuLm5ITg4uEyHPRWlffv2SEhI0Cm7fv066tatCwBwd3eHk5MToqOjtYmEUqnEiRMn8Omnn760XblcXuRqVqampjA1NS27D0BEVYsmV7/6vN8QEZHE6fNsXKo5FjNnzizNy/QWGBiIdu3aYe7cuRg0aBBOnjyJFStWYMWKp8MNZDIZJk2ahNmzZ8PDw0O73KyLiwv69+9fITESEREREZEeiUWLFi0QHR2N6tWro3nz5kVO3n7m7NmzZRJc69atsX37dkyfPh0RERFwd3fHwoULMXToUG2dqVOn4vHjx/j444+RmZmJDh06YO/evdzDgoiIiIioApU4sejXr592+FBF9gb85z//wX/+85+XnpfJZIiIiEBERESFxURERERERLpKtUHe64Yb5BFRmeAGeURE9Jop9w3yiIiIiIiInlfioVDVq1cvdl7F84ralZuIqDJym7a7xHWTObWLiIiqsBInFgsXLizHMIiIiIiIqDIrcWJhyE7XRERERET0eitxYqFUKrUTNl61Ax8nQBMRERERVS16zbFIS0uDg4MDbG1ti5xvIQgCZDIZCgoKyjRIIiIiIiKSthInFn/++Sfs7OwAAIcOHSq3gIiIiIiIqPIpcWLRuXPnIr8nIiIiIiIqcWLxoocPH2LVqlW4evUqAKBx48YYMWKEtleDiIiIiIiqjlJtkBcXFwc3NzcsWrQIDx8+xMOHD7Fo0SK4u7sjLi6urGMkIiIiIiKJK1WPxdixY/H+++9j6dKlMDY2BgAUFBTgs88+w9ixY3Hp0qUyDZKIiIiIiKStVD0WiYmJmDx5sjapAABjY2MEBQUhMTGxzIIjIiIiIqLKoVSJRYsWLbRzK5539epVeHt7GxwUERERERFVLiUeCnXx4kXt9xMmTMDEiRORmJiItm3bAgCOHz+OxYsXY968eWUfJRERERERSZpMEAShJBWNjIwgk8nwquqVcYM8pVIJhUKBrKws7hpORDrcpu0ucd1k8w/0azwsS89oiIiIKpY+z8kl7rFISkoyODAiIiIiIno9lTixqFu3bnnGQURERERElVipN8gDgCtXriAlJQV5eXk65e+8845BQRERERERUeVSqsTi1q1bePfdd3Hp0iWdeRcymQwAymyORVhYGMLDw3XKGjZsiGvXrgEAcnNzMXnyZGzZsgUqlQp+fn5YsmQJHB0dy+T9iYiIiIioZEq13OzEiRPh7u6Ou3fvwtLSEn/99Rfi4uLQqlUrxMTElGmATZo0QVpamvbryJEj2nOBgYHYuXMnfv75Z8TGxuLOnTvw9/cv0/cnIiIiIqJXK1WPRXx8PP7880/UrFkTRkZGMDIyQocOHRAZGYkJEybg3LlzZRegiQmcnJwKlWdlZWHVqlXYvHkzunXrBgBYs2YNPD09cfz4ce0yuEREREREVP5KlVgUFBTA2toaAFCzZk3cuXMHDRs2RN26dZGQkFCmAd64cQMuLi4wNzeHj48PIiMjUadOHZw5cwZqtRq+vr7auo0aNUKdOnUQHx9fbGKhUqmgUqm0x0qlEgCgVquhVqvLNH4iqtzkxiVakRsAoDYy169x3m+IiEji9Hk2LlVi8eabb+LChQtwd3dHmzZtEBUVBTMzM6xYsQL16tUrTZNFatOmDdauXYuGDRsiLS0N4eHh6NixIy5fvoz09HSYmZnB1tZW5zWOjo5IT08vtt3IyMhCczcAYP/+/bC0tCyz+Imo8ot6q+R192CFfo3v2aNffSIiogqWk5NT4rol3iDvefv27cPjx4/h7++PxMRE/Oc//8H169dRo0YNbN26VTs0qaxlZmaibt26mD9/PiwsLDBixAidngcAeOutt9C1a1d89dVXL22nqB4LV1dX/Pvvv9wgj4h0vBm2r8R1L8sD9Gt8+t96RkNERFSxlEolatasWbYb5D3Pz89P+32DBg1w7do1PHjwANWrV9euDFUebG1t8cYbbyAxMRFvv/028vLykJmZqdNrkZGRUeScjOfJ5XLI5fJC5aampjA1NS3rsImoElMVlPyeZqrJ1a9x3m+IiEji9Hk2LtWqUM9LTU1Famoq7OzsyjWpAIDs7GzcvHkTzs7OaNmyJUxNTREdHa09n5CQgJSUFPj4+JRrHEREREREpKtUiUV+fj5CQkKgUCjg5uYGNzc3KBQKzJgxo0wnPwcHByM2NhbJyck4duwY3n33XRgbG2PIkCFQKBQICAhAUFAQDh06hDNnzmDEiBHw8fHhilBERERERBWsVEOhxo8fj23btiEqKkrbOxAfH4+wsDDcv38fS5cuLZPg/v77bwwZMgT379+Hvb09OnTogOPHj8Pe3h4AsGDBAhgZGWHAgAE6G+QREREREVHFKtXkbYVCgS1btqBXr1465Xv27MGQIUOQlZVVZgFWBKVSCYVCUaJJKURUtbhN213iusnmH+jXeFjlulcSEVHVo89zcqmGQsnlcri5uRUqd3d3h5mZWWmaJCIiIiKiSqxUicW4ceMwa9YsnSVbVSoV5syZg3HjxpVZcEREREREVDmUeI6Fv7+/zvHBgwdRu3ZteHt7AwAuXLiAvLw8dO/evWwjJCIiIiIiyStxYqFQKHSOBwwYoHPs6upaNhEREREREVGlU+LEYs2aNeUZBxERERERVWKlWm72mXv37iEhIQEA0LBhQ+0ysEREREREVLWUavL248ePMXLkSDg7O6NTp07o1KkTXFxcEBAQgJycnLKOkYiIiIiIJK5UiUVQUBBiY2Oxc+dOZGZmIjMzE7/99htiY2MxefLkso6RiIiIiIgkrlRDoX799Vf88ssv6NKli7asd+/esLCwwKBBg8ps520iIiIiIqocStVjkZOTA0dHx0LlDg4OHApFRERERFQFlSqx8PHxwcyZM5Gbm6ste/LkCcLDw+Hj41NmwRERERERUeVQqqFQCxcuRM+ePQttkGdubo59+/aVaYBERERERCR9pUosvLy8cOPGDWzatAnXrl0DAAwZMgRDhw6FhYVFmQZIRERERETSp3dioVar0ahRI+zatQujR48uj5iIiIiIiKiS0XuOhampqc7cCiIiIiIiolJN3h47diy++uor5Ofnl3U8RERERERUCZVqjsWpU6cQHR2N/fv3w8vLC9WqVdM5v23btjIJjoiIiIiIKodSJRa2trYYMGBAWcdCRERERESVlF5DoTQaDb766itcv34dly9fhoODA5YsWYI1a9bofJWXefPmQSaTYdKkSdqy3NxcjB07FjVq1ICVlRUGDBiAjIyMcouBiIiIiIgK0yuxmDNnDr744gtYWVmhVq1aWLRoEcaOHVtesek4deoUli9fjqZNm+qUBwYGYufOnfj5558RGxuLO3fuwN/fv0JiIiIiIiKip/RKLNavX48lS5Zg37592LFjB3bu3IlNmzZBo9GUV3wAgOzsbAwdOhQrV65E9erVteVZWVlYtWoV5s+fj27duqFly5ZYs2YNjh07huPHj5drTERERERE9P/plVikpKSgd+/e2mNfX1/IZDLcuXOnzAN73tixY9GnTx/4+vrqlJ85cwZqtVqnvFGjRqhTpw7i4+PLNSYiIiIiIvr/9Jq8nZ+fD3Nzc50yU1NTqNXqMg3qeVu2bMHZs2dx6tSpQufS09NhZmYGW1tbnXJHR0ekp6e/tE2VSgWVSqU9ViqVAJ5u/leen4WIKh+5sVDiumoj81dX0nkB7zdERCRt+jwb65VYCIKA4cOHQy6Xa8tyc3MxZswYnSVny2q52dTUVEycOBEHDhwolNAYIjIyEuHh4YXK9+/fD0tLyzJ7HyKq/KLeKnndPVihX+N79uhXn4iIqILl5OSUuK5MEIQS/zluxIgRJapXVitD7dixA++++y6MjY21ZQUFBZDJZDAyMsK+ffvg6+uLhw8f6vRa1K1bF5MmTUJgYGCR7RbVY+Hq6op///0XNjY2ZRI7Eb0e3gzbV+K6l+UB+jU+/W89oyEiIqpYSqUSNWvWRFZW1iufk/XqsSjPpWSL0r17d1y6dEmnbMSIEWjUqBE+//xzuLq6wtTUFNHR0dp9NRISEpCSkgIfH5+XtiuXy3V6XZ4xNTWFqalp2X4IIqrUVAWyEtc11eTq1zjvN0REJHH6PBuXaoO8imJtbY0333xTp6xatWqoUaOGtjwgIABBQUGws7ODjY0Nxo8fDx8fH7Rt21aMkImIiIiIqiRJJxYlsWDBAhgZGWHAgAFQqVTw8/PDkiVLxA6LiIiIiKhK0WuOxetKqVRCoVCUaOwYEVUtbtN2l7husvkH+jUelqVnNERERBVLn+dkvfaxICIiIiIiKgoTCyIiIiIiMhgTCyIiIiIiMhgTCyIiIiIiMhgTCyIiIiIiMhgTCyIiIiIiMlil38eCiIiIiIqnz9LZAJA8r085RUKvM/ZYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwZhYEBERERGRwSSdWCxduhRNmzaFjY0NbGxs4OPjgz/++EN7Pjc3F2PHjkWNGjVgZWWFAQMGICMjQ8SIiYiIiIiqJkknFrVr18a8efNw5swZnD59Gt26dUO/fv3w119/AQACAwOxc+dO/Pzzz4iNjcWdO3fg7+8vctRERERERFWPidgBFKdv3746x3PmzMHSpUtx/Phx1K5dG6tWrcLmzZvRrVs3AMCaNWvg6emJ48ePo23btmKETERERERUJUk6sXheQUEBfv75Zzx+/Bg+Pj44c+YM1Go1fH19tXUaNWqEOnXqID4+vtjEQqVSQaVSaY+VSiUAQK1WQ61Wl9+HIKJKR24slLiu2shcv8Z5vyGiCqLPvQwAn4dIS5/fBcknFpcuXYKPjw9yc3NhZWWF7du3o3Hjxjh//jzMzMxga2urU9/R0RHp6enFthkZGYnw8PBC5fv374elpWVZhk9ElVzUWyWvuwcr9Gt8zx796hMRlZI+9zIA2MP7E/2fnJycEteVfGLRsGFDnD9/HllZWfjll18wbNgwxMbGGtTm9OnTERQUpD1WKpVwdXVFjx49YGNjY2jIRPQaeTNsX4nrXpYH6Nf49L/1jIaIqHT0uZcBwOUwv3KKhCqbZyN7SkLyiYWZmRkaNGgAAGjZsiVOnTqF7777Du+//z7y8vKQmZmp02uRkZEBJyenYtuUy+WQy+WFyk1NTWFqalqm8RNR5aYqkJW4rqkmV7/Geb8hogqiz70MAJ+HSEuf3wVJrwpVFI1GA5VKhZYtW8LU1BTR0dHacwkJCUhJSYGPj4+IERIRERERVT2S7rGYPn06evXqhTp16uDRo0fYvHkzYmJisG/fPigUCgQEBCAoKAh2dnawsbHB+PHj4ePjwxWhiIiIiIgqmKQTi7t37+Kjjz5CWloaFAoFmjZtin379uHtt98GACxYsABGRkYYMGAAVCoV/Pz8sGTJEpGjJiIiIiKqeiSdWKxatarY8+bm5li8eDEWL15cQREREREREVFRKt0cCyIiIiIikh4mFkREREREZDAmFkREREREZDAmFkREREREZDAmFkREREREZDAmFkREREREZDAmFkREREREZDAmFkREREREZDAmFkREREREZDAmFkREREREZDATsQMgIqqqvNZ5lbjupWGXyjESIiIiw7HHgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMbEgoiIiIiIDMblZomIiIio1PRZOhvg8tmvM0n3WERGRqJ169awtraGg4MD+vfvj4SEBJ06ubm5GDt2LGrUqAErKysMGDAAGRkZIkVMRERERFQ1STqxiI2NxdixY3H8+HEcOHAAarUaPXr0wOPHj7V1AgMDsXPnTvz888+IjY3FnTt34O/vL2LURERERERVj6SHQu3du1fneO3atXBwcMCZM2fQqVMnZGVlYdWqVdi8eTO6desGAFizZg08PT1x/PhxtG3bVoywiYiIiIiqHEn3WLwoKysLAGBnZwcAOHPmDNRqNXx9fbV1GjVqhDp16iA+Pl6UGImIiIiIqiJJ91g8T6PRYNKkSWjfvj3efPNNAEB6ejrMzMxga2urU9fR0RHp6ekvbUulUkGlUmmPlUolAECtVkOtVpd98ERUacmNhRLXVRuZ69c25CVvm/cmIjKAPvcyQL97jj73Mn3bJvHp8/OqNInF2LFjcfnyZRw5csTgtiIjIxEeHl6ofP/+/bC0tDS4fSJ6fUS9VfK6e7BCr7ZD9Ki7Z88evdomInqePvcyQL97ToitPncz3s8qm5ycnBLXrRSJxbhx47Br1y7ExcWhdu3a2nInJyfk5eUhMzNTp9ciIyMDTk5OL21v+vTpCAoK0h4rlUq4urqiR48esLGxKZfPQESV05th+0pc97I8QK+2feq6lrhu/Acc3klEpafPvQwALof5lbiuz2Yfvdrm/axyeTaypyQknVgIgoDx48dj+/btiImJgbu7u875li1bwtTUFNHR0RgwYAAAICEhASkpKfDxefkvuVwuh1xeuNvO1NQUpqamZfshiKhSUxXISlzXVJOrX9tQvbrSs7Z5byIiA+hzLwP0u+focy/Tt20Snz4/L0knFmPHjsXmzZvx22+/wdraWjtvQqFQwMLCAgqFAgEBAQgKCoKdnR1sbGwwfvx4+Pj4cEUoIiIiIqIKJOnEYunSpQCALl266JSvWbMGw4cPBwAsWLAARkZGGDBgAFQqFfz8/LBkyZIKjpSIiIiIqGqTdGIhCK9ewcDc3ByLFy/G4sWLKyAiIiIiIiIqSqXax4KIiIiIiKSJiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERlM0vtYUNnwWuelV/1Lwy6VUyRERERE9LpijwURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERmMiQURERERERlM8olFXFwc+vbtCxcXF8hkMuzYsUPnvCAICA0NhbOzMywsLODr64sbN26IEywRERERURUl+cTi8ePH8Pb2xuLFi4s8HxUVhUWLFmHZsmU4ceIEqlWrBj8/P+Tm5lZwpEREREREVZeJ2AG8Sq9evdCrV68izwmCgIULF2LGjBno168fAGD9+vVwdHTEjh07MHjw4IoMlYiIiIioypJ8YlGcpKQkpKenw9fXV1umUCjQpk0bxMfHvzSxUKlUUKlU2mOlUgkAUKvVUKvV5Ru0COSQ61X/dbwGRKUlNxZKXFdtZK5f23r82+S/SyIyhD73MkC/ew6fM15v+vy8ZIIg6PebJiKZTIbt27ejf//+AIBjx46hffv2uHPnDpydnbX1Bg0aBJlMhq1btxbZTlhYGMLDwwuVb968GZaWluUSOxERERFRZZOTk4MPPvgAWVlZsLGxKbZupe6xKK3p06cjKChIe6xUKuHq6ooePXq88oJVRj6bffSqH/9BfDlFQlT5vBm2r8R1L8sD9Grbp65rievy3yURGUKfexkAXA7zK3FdPme83p6N7CmJSp1YODk5AQAyMjJ0eiwyMjLQrFmzl75OLpdDLi/cbWdqagpTU9Myj1NsKqheXek5r+M1ICotVYGsxHVNNfotGqHPv03+uyQiQ+hzLwP0u+fwOeP1ps/Pq1InFu7u7nByckJ0dLQ2kVAqlThx4gQ+/fRTcYMjIiIiIiqC1zovvepfGnapnCIpW5JPLLKzs5GYmKg9TkpKwvnz52FnZ4c6depg0qRJmD17Njw8PODu7o6QkBC4uLho52EQEdHrQ5//GVeW/xETEb0uJJ9YnD59Gl27dtUeP5sbMWzYMKxduxZTp07F48eP8fHHHyMzMxMdOnTA3r17YW6u3+osRERERERUepJPLLp06YLiFq6SyWSIiIhAREREBUZFRERERETPk/zO20REREREJH2S77EgEsPrOqmKiIiIqLywx4KIiIiIiAzGxIKIiIiIiAzGxIKIiIiIiAzGORYS4TZtt171k+f1KadIiIiIiIj0xx4LIiIiIiIyGBMLIiIiIiIyGBMLIiIiIiIyGOdYEBERvaa4Jw8RVSQmFkRUJvR5gOHDCxFJFZMxotLjUCgiIiIiIjIYEwsiIiIiIjIYEwsiIiIiIjIYEwsiIiIiIjIYEwsiIiIiIjIYEwsiIiIiIjIYEwsiIiIiIjLYa5NYLF68GG5ubjA3N0ebNm1w8uRJsUMiIiIiIqoyXovEYuvWrQgKCsLMmTNx9uxZeHt7w8/PD3fv3hU7NCIiIiKiKuG12Hl7/vz5GD16NEaMGAEAWLZsGXbv3o3Vq1dj2rRpIkdHREREVMmEKUpe171O+cVBlUqlTyzy8vJw5swZTJ8+XVtmZGQEX19fxMfHixgZEREREVVWbtN2l7hu8rw+5RhJ5VHpE4t///0XBQUFcHR01Cl3dHTEtWvXinyNSqWCSqXSHmdlZQEAHjx4ALVaXX7BFsMk/7Fe9e/fv1/ytp/o92PWp20xtYmM1qv+iendS1z3db1m5Umfa1aZrpc+/zbv55np1/Zres30+bepz79L4PW9ZuWF9zL9va7XTO/nDD3uZ7xm+n+mynTNHj16BAAQBOGVdWVCSWpJ2J07d1CrVi0cO3YMPj4+2vKpU6ciNjYWJ06cKPSasLAwhIeHV2SYRERERESVVmpqKmrXrl1snUrfY1GzZk0YGxsjIyNDpzwjIwNOTk5Fvmb69OkICgrSHms0Gjx48AA1atSATCYr13j1oVQq4erqitTUVNjY2IgdTqXAa6Y/XjP98Hrpj9dMf7xm+uM10x+vmf6q4jUTBAGPHj2Ci4vLK+tW+sTCzMwMLVu2RHR0NPr37w/gaaIQHR2NcePGFfkauVwOuVyuU2Zra1vOkZaejY1NlfnlLSu8ZvrjNdMPr5f+eM30x2umP14z/fGa6a+qXTOFomST+St9YgEAQUFBGDZsGFq1aoW33noLCxcuxOPHj7WrRBERERERUfl6LRKL999/H/fu3UNoaCjS09PRrFkz7N27t9CEbiIiIiIiKh+vRWIBAOPGjXvp0KfKSi6XY+bMmYWGbdHL8Zrpj9dMP7xe+uM10x+vmf54zfTHa6Y/XrPiVfpVoYiIiIiISHxGYgdARERERESVHxMLIiIiIiIyGBMLIiIiIiIyGBMLIiIiIiIyGBMLCcnPz8f69esL7SJORERERCR1XBVKYiwtLXH16lXUrVtX7FAqhWHDhiEgIACdOnUSO5RKo169ejh16hRq1KihU56ZmYkWLVrg1q1bIkUmLb///nuJ677zzjvlGAlVVQUFBbh06RLq1q2L6tWrix0OVVJKpbLEdavSTtIlFRcXV+x5Pn/oem32sXhdvPXWWzh//jwTixLKysqCr68v6tatixEjRmDYsGGoVauW2GFJWnJyMgoKCgqVq1Qq/PPPPyJEJE39+/fXOZbJZHj+7zAymUz7fVHXk4B169ahZs2a6NOnDwBg6tSpWLFiBRo3bowff/yR97kXTJo0CV5eXggICEBBQQE6d+6MY8eOwdLSErt27UKXLl3EDpEqIVtbW537VXF4LyusqH93vP+/HBMLifnss88QFBSE1NRUtGzZEtWqVdM537RpU5Eik6YdO3bg3r172LBhA9atW4eZM2fC19cXAQEB6NevH0xNTcUOUTKe/wv8vn37oFAotMcFBQWIjo6Gm5ubCJFJk0aj0X5/8OBBfP7555g7dy58fHwAAPHx8ZgxYwbmzp0rVoiSN3fuXCxduhTA0+u1ePFiLFiwALt27UJgYCC2bdsmcoTS8ssvv+DDDz8EAOzcuRNJSUm4du0aNmzYgC+//BJHjx4VOUJp+uWXX/DTTz8hJSUFeXl5OufOnj0rUlTScejQIe33ycnJmDZtGoYPH65zL1u3bh0iIyPFClHSHj58qHOsVqtx7tw5hISEYM6cOSJFJWECSYpMJiv0ZWRkpP0vFe/MmTPCuHHjBHNzc6FmzZrCpEmThOvXr4sdliQU9bv17MvMzEx44403hJ07d4odpiQ1adJEOHz4cKHyuLg4oVGjRiJEVDlYWFgIt2/fFgRBEKZOnSr897//FQRBEC5fvizUrFlTzNAkSS6XC6mpqYIgCMLo0aOFiRMnCoIgCLdu3RKsra1FjEy6vvvuO8HKykoYN26cYGZmJnzyySeCr6+voFAohC+++ELs8CSnW7duwubNmwuVb9q0SejcuXPFB1SJxcTECC1atBA7DMnh5G2JSUpKKvR169Yt7X/p5dLS0nDgwAEcOHAAxsbG6N27Ny5duoTGjRtjwYIFYocnOo1GA41Gg7p16+LevXvaY41GA5VKhYSEBPznP/8RO0xJunnzJmxtbQuVKxQKJCcnV3g8lYWVlRXu378PANi/fz/efvttAIC5uTmePHkiZmiS5OjoiCtXrqCgoAB79+7VXq+cnBwYGxuLHJ00LVmyBCtWrMD//vc/mJmZYerUqThw4AAmTJiArKwsscOTnPj4eLRq1apQeatWrXDy5EkRIqq8HB0dkZCQIHYYksOhUBLDMcf6UavV+P3337FmzRrs378fTZs2xaRJk/DBBx9oJ6Ft374dI0eORGBgoMjRik+tVqNevXp48OBBocnb9HKtW7dGUFAQNmzYAEdHRwBARkYGpkyZgrfeekvk6KTr7bffxqhRo9C8eXNcv34dvXv3BgD89ddfHHZXhBEjRmDQoEFwdnaGTCaDr68vAODEiRNo1KiRyNFJU0pKCtq1awcAsLCwwKNHjwAA//3vf9G2bVt8//33YoYnOa6urli5ciWioqJ0yn/44Qe4urqKFJW0Xbx4UedYEASkpaVh3rx5aNasmThBSRgTCwnasGEDli1bhqSkJMTHx6Nu3bpYuHAh3N3d0a9fP7HDkxRnZ2doNBoMGTIEJ0+eLPIfedeuXYv8a3NVZGpqWugmSa+2atUq+Pv7o06dOtr/+aampsLDwwM7duwQNzgJW7x4MWbMmIHU1FT8+uuv2mT2zJkzGDJkiMjRSU9YWBjefPNNpKamYuDAgZDL5QAAY2NjTJs2TeTopMnJyQkPHjxA3bp1UadOHRw/fhze3t5ISkrSWWyBnlqwYAEGDBiAP/74A23atAEAnDx5Ejdu3MCvv/4qcnTS1KxZs0KLdwBA27ZtsXr1apGiki4uNysxS5cuRWhoKCZNmoQ5c+bg8uXLqFevHtauXYt169bpTMKip0nYwIEDYW5uLnYolUZgYCDkcjnmzZsndiiViiAIOHDgAK5duwYA8PT0hK+vb4lXWyHSR25uLu9rJTBq1Ci4urpi5syZWLx4MaZMmYL27dvj9OnT8Pf3x6pVq8QOUXL+/vtvLF26FFevXgXw9F42ZswY9li8xO3bt3WOjYyMYG9vz3+fL8HEQmIaN26MuXPnon///rC2tsaFCxdQr149XL58GV26dMG///4rdoiSoVarYWFhgfPnz+PNN98UO5xKY/z48Vi/fj08PDyKXHls/vz5IkUmTfw9M8zhw4exfPly3Lp1Cz///DNq1aqFDRs2wN3dHR06dBA7PEkpKCjA3LlzsWzZMmRkZOD69euoV68eQkJC4ObmhoCAALFDlJxn88RMTJ4OwNiyZQuOHTsGDw8PfPLJJzAzMxM5QulQq9Xo2bMnli1bBg8PD7HDodcUJ29LTFJSEpo3b16oXC6X4/HjxyJEJF2mpqaoU6cO15DW0+XLl9GiRQtYW1vj+vXrOHfunPbr/PnzYocnOfw9K71ff/0Vfn5+sLCwwNmzZ6FSqQA83X+Gy/QWNmfOHKxduxZRUVE6D8RvvvkmfvjhBxEjky4jIyNtUgEAgwcPxqJFizB+/HgmFS/gUNjSi42NRd++fdGgQQM0aNAA77zzDg4fPix2WJLExEJi3N3di3y427t3Lzw9PSs+IIn78ssv8cUXX+DBgwdih1JpHDp06KVff/75p9jhSRJ/z0pn9uzZWLZsGVauXKmzp0z79u25v0AR1q9fjxUrVmDo0KE6q0B5e3trh+CRrnr16mHEiBHapPWZf//9F/Xq1RMpKun68MMPOTxMTxs3boSvry8sLS0xYcIETJgwARYWFujevTs2b94sdniSw8nbEhMUFISxY8ciNzcXgiDg5MmT+PHHHxEZGcm/WBXh+++/R2JiIlxcXFC3bt1Cw3r48FK8v//+GwBQu3ZtkSORNv6elU5CQgI6depUqFyhUCAzM7PiA5K4f/75Bw0aNChUrtFooFarRYhI+pKTk2FiYoKOHTvi999/h5OTE4Cnw8peHBtPQH5+PlavXo2DBw9yKGwJzZkzB1FRUTorS06YMAHz58/HrFmz8MEHH4gYnfQwsZCYUaNGwcLCAjNmzEBOTg4++OADuLi44LvvvsPgwYPFDk9y+vfvL3YIlY5Go8Hs2bPx7bffIjs7GwBgbW2NyZMn48svv4SRETsyX8Tfs9JxcnJCYmJioaVljxw5wr8mF6Fx48Y4fPhwoWXHf/nllyKHyBIgk8mwd+9eBAcHo2XLltixYwdat24tdliS9WwoLABcv35d5xwXoijarVu30Ldv30Ll77zzDr744gsRIpI2JhYSNHToUAwdOhQ5OTnIzs6Gg4OD2CFJ1syZM8UOodL58ssvsWrVKsybNw/t27cH8PRBLywsDLm5uZgzZ47IEUoPf89KZ/To0Zg4cSJWr14NmUyGO3fuID4+HsHBwQgJCRE7PMkJDQ3FsGHD8M8//0Cj0WDbtm1ISEjA+vXrsWvXLrHDkyRBEGBlZYVt27Zh+vTp6Ny5M1asWKHdXJB0cWVJ/bm6uiI6OrpQb+LBgwe5klZRxNrym4qWk5MjPH78WHucnJwsLFiwQNi3b5+IUUnbw4cPhZUrVwrTpk0T7t+/LwiCIJw5c0b4+++/RY5MmpydnYXffvutUPmOHTsEFxcXESKi15VGoxFmz54tVKtWTZDJZIJMJhPMzc2FGTNmiB2aZMXFxQm+vr6Cvb29YGFhIbRv3573/2IYGRkJGRkZ2uMNGzYI5ubmwogRIwQjIyMRI6PXxZIlSwQzMzNhzJgxwvr164X169cLn3zyiSCXy4Vly5aJHZ7kcLlZienRowf8/f0xZswYZGZmomHDhjAzM8O///6L+fPn49NPPxU7REm5ePEifH19oVAokJycjISEBNSrVw8zZsxASkoK1q9fL3aIkmNubo6LFy/ijTfe0ClPSEhAs2bN8OTJE5Eik66CggIsWLAAP/30E1JSUpCXl6dznpO6i5eXl4fExERkZ2ejcePGsLKyEjskek0YGRkhPT1dp2c/Pj4e7777Lu7du8fV3Ipw+vTpl97Ltm3bJlJU0rZ9+3Z8++23Ont/TJkyhZsWF4GDqSXm7Nmz6NixI4Cn42qdnJxw+/ZtrF+/HosWLRI5OukJCgrC8OHDcePGDZ3Nanr37o24uDgRI5Mub29vfP/994XKv//+e3h7e4sQkfSFh4dj/vz5eP/995GVlYWgoCD4+/vDyMgIYWFhYocneWZmZmjcuDHeeustJhXFGDVqFGJiYsQOo1LRaDSFhgv7+PjgwoULXOWuCFu2bEG7du1w9epVbN++HWq1Gn/99Rf+/PNPKBQKscOTpGHDhqFGjRo4cuQI7t+/j/v37+PIkSNMKl6CcywkJicnB9bW1gCA/fv3ax9e2rZtyxUuinDq1CksX768UHmtWrWQnp4uQkTSFxUVhT59+uDgwYPw8fEB8PQvfKmpqdizZ4/I0UnTpk2bsHLlSvTp0wdhYWEYMmQI6tevj6ZNm+L48eOYMGGC2CFK0uPHjzFv3jxER0fj7t270Gg0Oudv3bolUmTSdO/ePfTs2RP29vYYPHgwhg4dimbNmokdlqRFRESgQ4cO6Natm065lZUVYmNj0blzZ5Eik6a5c+diwYIFGDt2LKytrfHdd9/B3d0dn3zyCZydncUOT5KysrLg6+uLunXrYsSIERg+fDhcXFzEDkuy2GMhMQ0aNMCOHTuQmpqKffv2oUePHgCAu3fvwsbGRuTopEcul0OpVBYqv379Ouzt7UWISPo6d+6M69ev491330VmZiYyMzPh7++PhIQEbW8Z6UpPT4eXlxeApw8sWVlZAID//Oc/2L17t5ihSdqoUaOwatUqdOzYEePGjcPEiRN1vkjXb7/9hrS0NISEhODUqVNo2bIlmjRpgrlz5yI5OVns8CQpLCwMvXr1KrRManZ2NsLDw0WKSrpu3ryJPn36AHjak/j48WPIZDIEBgZixYoVIkcnTTt27MA///yDTz/9FFu3bkXdunXRq1cv/Pzzz1wGuihiT/IgXT///LNgamoqGBkZCb6+vtryuXPnCj179hQxMmkKCAgQ+vfvL+Tl5QlWVlbCrVu3hNu3bwvNmzcXJk6cKHZ4kvHuu+8KWVlZgiAIwrp164Tc3FyRI6pc3njjDeH48eOCIAhC+/bthcjISEEQBGHLli2Cvb29mKFJmkKhEI4cOSJ2GJVWamqqEBUVJTRq1EgwNjYWOxxJkslkwpYtW4QaNWoIw4cPF1QqlSAIgpCens7J20WoVauWcPHiRUEQBMHLy0vYvHmzIAiCcOzYMcHGxkbM0CqNM2fOCOPGjRPMzc2FmjVrCpMmTRKuX78udliSwR4LiXnvvfeQkpKC06dPY9++fdry7t27Y8GCBSJGJk3P9mJwcHDAkydP0LlzZzRo0ADW1tZcNvU5u3btwuPHjwEAI0aM0P7FnUrm3XffRXR0NABg/PjxCAkJgYeHBz766COMHDlS5Oikq3r16rCzsxM7jEpJrVbj9OnTOHHiBJKTk+Ho6Ch2SJLVtWtXnDhxAidOnECXLl1w9+5dsUOSrE6dOuHAgQMAgIEDB2LixIkYPXo0hgwZgu7du4scnfSlpaXhwIEDOHDgAIyNjdG7d29cunQJjRs35jPa/+GqUBLGXZFL7siRI7h48SKys7PRokUL+Pr6ih2SpDRt2hQtWrRA165dMWLECCxatOilQ+s++uijCo6u8jl+/DiOHTsGDw+PIjdOoqc2btyI3377DevWrYOlpaXY4VQKhw4dwubNm/Hrr79Co9HA398fQ4cORbdu3biBWRGMjY2RlpYGBwcHKJVKDBo0CH/99ReWLVuGd955h6tCveDBgwfIzc2Fi4sLNBoNoqKitPeyGTNmoHr16mKHKDlqtRq///471qxZg/3796Np06YYNWoUPvjgA+3/R7dv346RI0fi4cOHIkcrPiYWEsNdkfWTmprKDWpK4OjRo5g8eTJu3ryJBw8ewNrausiHFJlMxqVTySDNmzfX+d1KTEyEIAhwc3ODqampTt2zZ89WdHiSVqtWLTx48AA9e/bE0KFD0bdvX8jlcrHDkrQXl5vVaDSYNGkSli5dCo1Gw8SCDFazZk1oNBoMGTIEo0ePLnJBhczMTDRv3hxJSUkVH6DEcFUoieGuyPpxc3NDhw4d8OGHH+K9997jX1teon379jh+/DiAp/8jvn79Ond010OdOnXQpUsXdO7cGV26dEH9+vXFDkmy+vfvL3YIlVZYWBgGDhwIW1tbsUOpNNasWaOzTKqRkREWLVqE5s2bc8nxInz00Ufo2rUrOnXqxPtYCS1YsAADBw7UWdL+Rba2tkwq/g97LCTGxcVF24X7vN9++w2fffYZ/vnnH5Eik6Zz585h8+bN2LJli3apxg8//JB/6XuBv78/1q5dCxsbG6xbtw6DBg2ChYWF2GFVGhs3bkRcXBxiYmKQmJiIWrVqoXPnztpEw8PDQ+wQ6TXDobBUHkaNGoW4uDid+9izP5rwPkZlgYmFxHBX5NIRBAExMTGFxiavXr1a7NAkwczMDLdv34azs7POmGTSX1paGmJjY7Fr1y5s3bqVwy2KcerUKWg0GrRp00an/MSJEzA2NkarVq1EikyaOBS2ZBYtWoSPP/4Y5ubmxW4cK5PJMH78+AqMrPL4559/EBcXh9jYWMTGxuL69etwdnbWJrREpcXEQmLatGmDNm3aFLpZjh8/HqdOndIOZ6GXO3v2LAICAnDx4kU+8P0fTt42XE5ODo4cOYKYmBgcOnQI586dg6enJ7p06cLVQF7irbfewtSpU/Hee+/plG/btg1fffUVTpw4IVJk0jR9+nSsWrUK4eHhhYbCjh49mkNh/4+7uztOnz6NGjVqwN3d/aX1ZDIZN2F8iWf3s0OHDiEmJgZnz55F48aNce7cObFDo0qOiYXExMbGok+fPqhTp06RuyJzA7Oi/f3339i8eTM2b96My5cvw8fHB0OHDsWYMWPEDk0Sjh07hqCgIE7eLqV27drpJBKdO3dGp06dOKfnFaysrHDx4kXUq1dPpzwpKQlNmzbFo0ePRIpMmjgU1jDPHme4etbLffHFF4iJidHez54NheL9jMoKEwsJunPnDhYvXoxr164BADw9PfHZZ59xC/kiLF++HJs3b8aRI0fg6emJoUOH4oMPPkDdunXFDk2yXlxFhV7Nzs4ORkZG6NGjB7p06YIuXboUGq5IhdWoUQO7du3S/pHkmWPHjqFPnz5cmvEFHApbOqtWrcKCBQtw48YNAICHhwcmTZqEUaNGiRyZ9BgZGcHe3h6BgYHw9/fnfYzKHBMLqtRcXV0xZMgQDB06FN7e3mKHUyncvn0bKSkpWL58OW7duoWff/4ZtWrVwoYNG+Du7o4OHTqIHaLkCIKAS5cuISYmBrGxsYiLi4OZmRk6d+6Mrl27YvTo0WKHKElDhgxBWloafvvtN+3KPZmZmejfvz8cHBzw008/iRyhtHAorP5CQ0Mxf/58jB8/XqeX//vvv0dgYCAiIiJEjlBaLly4gNjYWMTExODw4cPa+xj/YEJlhYmFBFy8eLHEdZs2bVqOkVQ+giDgyJEjfEjWw6+//or//ve/GDp0KDZs2IArV66gXr16+P7777Fnzx7s2bNH7BAlTRAEnDlzBt9//z02bdrEydvF+Oeff9CpUyfcv38fzZs3BwCcP38ejo6OOHDgAPegecHLhsKmpKTgjz/+4FDYItjb22PRokUYMmSITvmPP/6I8ePH499//xUpssrhwoULWLBgAe9lVGa4j4UENGvWDDKZDK/K8WQyGf/Rv2Dbtm3ah+SzZ89CpVIBALKysjB37lw+JBdh9uzZWLZsGT766CNs2bJFW96+fXvMnj1bxMik6+zZs4iJiUFMTAyOHDmCR48ewcvLC+PHj0fnzp3FDk+yatWqhYsXL2LTpk24cOECLCwsMGLECAwZMqTQZnkEdO7cGQkJCVi6dCmuXr0K4OlS0RwK+3JqtbrI1cVatmyJ/Px8ESKSNkEQcO7cOZ37mVKpRNOmTXkvozLBHgsJuH37donrcu6ArubNmyMwMBAfffQRrK2tceHCBdSrVw/nzp1Dr169kJ6eLnaIkmNpaYkrV67Azc1N55rdunULjRs3Rm5urtghSo6JiQmaN2+u3buiU6dOOptyEZWV3NxcXLx4EXfv3oVGo9E59+Kkbno6TMzU1BTz58/XKQ8ODsaTJ0+wePFikSKTpurVqyM7Oxve3t7aIVAdO3bkpoxUZthjIQHPJwuRkZFwdHTEyJEjdeqsXr0a9+7dw+eff17R4UlaQkICOnXqVKhcoVAgMzOz4gOqBJycnJCYmAg3Nzed8iNHjhRavYeAgoICbNu2DR07duSqKaVw48YNHDp0qMgH5dDQUJGikqa9e/fio48+wv379wv1YLPH+uVWrVqF/fv3o23btgCe7pOSkpKCjz76CEFBQdp6LyYfVdHGjRvRsWPHly43TmQoJhYS82yVoxc1adIEgwcPZmLxAj4k62/06NGYOHEiVq9eDZlMhjt37iA+Ph7BwcEICQkROzzJMTY2xqBBg3D16lUmFnpauXIlPv30U9SsWRNOTk46y4DKZDImFi8YP348Bg4ciNDQUDg6OoodTqVw+fJltGjRAgBw8+ZNAEDNmjVRs2ZNXL58WVuPS9A+1adPH+333N2dygMTC4lJT0+Hs7NzoXJ7e3ukpaWJEJG08SFZf9OmTYNGo0H37t2Rk5ODTp06QS6XIzg4mLvUvsSbb76JW7duFbsZFxU2e/ZszJkzh38QKaGMjAwEBQUxqdDDoUOHxA6hUuHu7lTemFhIjKurK44ePVroAebo0aOcvFcEPiTrTyaT4csvv8SUKVOQmJiI7OxsNG7cGFZWVmKHJlmzZ89GcHAwZs2ahZYtW6JatWo65zmsoGgPHz7EwIEDxQ6j0njvvfcQExOD+vXrix0Kvaa+/PJLrFq1CvPmzSu0u3tubi53dyeDcfK2xERFRSEqKgpff/01unXrBgCIjo7G1KlTMXnyZEyfPl3kCKUpLy+PD8lUbp7/K97zQyoEQeDY92IEBASgdevWGDNmjNihVAo5OTkYOHAg7O3t4eXlVWjlrAkTJogUGb0uuLs7lTf2WEjMlClTcP/+fXz22WfIy8sD8HQ31s8//5xJRTHMzMzQuHFjscOg1xSHW5ROgwYNEBISguPHj/NBuQR+/PFH7N+/H+bm5oiJiSk0J4XXiwz14MEDNGrUqFB5o0aN8ODBAxEiotcNeywkKjs7G1evXoWFhQU8PDwgl8vFDomISC/FzUmRyWS4detWBUYjfU5OTpgwYQKmTZvGse5ULri7O5U3JhZERCWQmZmJVatWaTcua9KkCUaOHMn9LKjM2NnZ4dSpU5xjQeXmZbu7p6amYs+ePdzdnQzGxIKI6BVOnz4NPz8/WFhY4K233gIAnDp1Ck+ePMH+/fu1y10SEBQUhFmzZqFatWo6ewi8SCaT4dtvv63AyKQvMDAQ9vb2+OKLL8QOhV5TKSkpMDExweLFi3Ht2jUAgKenJz777DPk5+ejTp06IkdIlR0TCyKiV+jYsSMaNGiAlStXwsTk6dS0/Px8jBo1Crdu3UJcXJzIEUpH165dsX37dtja2qJr164vrSeTyfDnn39WYGTSN2HCBKxfvx7e3t5o2rRpoTkp3OCNDGVsbIy0tDQ4ODjolN+/fx8ODg5ciIIMxsSCiOgVLCwscO7cuUKTHq9cuYJWrVohJydHpMjodcJEjMqbkZER0tPTCyUWt2/fRuPGjfH48WORIqPXBVeFIiJ6BRsbG6SkpBRKLFJTU2FtbS1SVPS64epjVF6eDUt8tuO9paWl9lxBQQFOnDiBZs2aiRQdvU6YWBARvcL777+PgIAAfPPNN2jXrh2Ap5tWTpkyBUOGDBE5OiKi4p07dw7A0713Ll26BDMzM+05MzMzeHt7Izg4WKzw6DXCoVBEREW4ePEi3nzzTRgZGSEvLw9TpkzBsmXLkJ+fDwAwNTXFp59+innz5nE5aCKqFEaMGIHvvvsONjY2YodCrykmFkRERXh+kmO9evVw6tQpWFhY4ObNmwCA+vXr6wwnICIiquo4FIqIqAi2trZISkqCg4MDkpOTodFoYGlpCS8vL7FDIyIikiQmFkRERRgwYAA6d+4MZ2dnyGQytGrVCsbGxkXW5Q7SRERETCyIiIq0YsUK+Pv7IzExERMmTMDo0aO5AhQREVExOMeCiOgVRowYgUWLFjGxICIiKgYTCyIiIiIiMpiR2AEQEREREVHlx8SCiIiIiIgMxsSCiIiIiIgMxsSCiIiIiIgMxsSCiIiIiIgMxsSCiIiIiIgMxsSCiIiIiIgMxsSCiIiIiIgM9v8Aw5vT0rhq7ikAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(\n",
    "        x + i * bar_width, 100 * scaled_probas[i], bar_width, label=f\"Temperature = {T}\"\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "ax.set_yticks(100 * np.arange(0.0, 1.1, 0.1))\n",
    "ax.grid(axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0430)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp5_idx = 2\n",
    "pizza_idx = 6\n",
    "\n",
    "scaled_probas[temp5_idx][pizza_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> RAM usage: 25.40%\n"
     ]
    }
   ],
   "source": [
    "print(f\">>> RAM usage: {psutil.virtual_memory().percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      ">>> Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\">>> Top logits:\", top_logits)\n",
    "print(\">>> Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits,\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5100,  0.8900, -1.9000,  6.7500,  1.6300, -1.6200, -1.8900,  6.2800,\n",
       "         1.7900])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    idx,\n",
    "    max_new_tokens,\n",
    "    context_size,\n",
    "    temperature=0.0,\n",
    "    top_k=None,\n",
    "    eos_id=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Listing 5.4 A modified text generation function with more diversity.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\">>> IDX: {idx}; shape: {idx.shape}\")\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step.\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\">>> LOGITS shape: {logits.shape}\")\n",
    "\n",
    "        # Last time step logits (batch_size, context_len, vocab_size) -> (batch_size, vocab_size).\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling.\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values.\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                condition=logits < min_val,\n",
    "                input=torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                other=logits,\n",
    "            )\n",
    "\n",
    "        # New: Apply temperature scaling.\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities.\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, vocab_size)\n",
    "\n",
    "            # Sample from the distribution.\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value.\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        # Stop generating early if end-of-sequence token is encountered and eos_id is specified.\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence.\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens + 1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Output text:\n",
      " Every effort moves you know began to my surprise, a little it was the\n",
      "\"Ah enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4,\n",
    ")\n",
    "\n",
    "print(\">>> Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Experiment 1/5 with Topk=3 and Temperature=1.4 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the fact with the last\n",
      "\n",
      ">>> Experiment 2/5 with Topk=3 and Temperature=1.4 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--I glanced after him the irony. She wanted\n",
      "\n",
      ">>> Experiment 3/5 with Topk=3 and Temperature=1.4 ...\n",
      "Every effort moves you say to put it all the axioms he laid down across the background\n",
      "\n",
      ">>> Experiment 4/5 with Topk=3 and Temperature=1.4 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 5/5 with Topk=3 and Temperature=1.4 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"\n",
      "\n",
      "\"\n",
      "I felt able to the the irony. She wanted\n",
      "\n",
      ">>> Experiment 1/5 with Topk=25 and Temperature=1.4 ...\n",
      "Every effort moves you sunbur inevitable not till my hostess was \"-- seen a tips of\n",
      "\n",
      ">>> Experiment 2/5 with Topk=25 and Temperature=1.4 ...\n",
      "Every effort moves you in the inevitable garlanded of my dear felt nervous so told me in\n",
      "\n",
      ">>> Experiment 3/5 with Topk=25 and Temperature=1.4 ...\n",
      "Every effort moves you ever\n",
      "\n",
      "\"Yes--quite. Gisburn--as such--\n",
      "\n",
      ">>> Experiment 4/5 with Topk=25 and Temperature=1.4 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "Of and my hostess answered with a dep or to the\n",
      "\n",
      ">>> Experiment 5/5 with Topk=25 and Temperature=1.4 ...\n",
      "Every effort moves you know with random up-chairs wild--I moved, at the equanim\n",
      "\n",
      ">>> Experiment 1/5 with Topk=75 and Temperature=1.4 ...\n",
      "Every effort moves you thought Jack Gisburn rather a cheap'ty't--as; not\n",
      "\n",
      ">>> Experiment 2/5 with Topk=75 and Temperature=1.4 ...\n",
      "Every effort moves you frame?\"\n",
      "For the marble her were, and't seen a picture '\n",
      "\n",
      ">>> Experiment 3/5 with Topk=75 and Temperature=1.4 ...\n",
      "Every effort moves you?\"\n",
      "The and; the picture for?--but me. Stroud\n",
      "\n",
      ">>> Experiment 4/5 with Topk=75 and Temperature=1.4 ...\n",
      "Every effort moves youementconf bitterness, Rick now how he answered with a little good fellow?\"\n",
      "\n",
      ">>> Experiment 5/5 with Topk=75 and Temperature=1.4 ...\n",
      "Every effort moves you of beauty through put it be place himself one they, struck by \" enough\n",
      "\n",
      ">>> Experiment 1/5 with Topk=250 and Temperature=1.4 ...\n",
      "Every effort moves youityating ' think's by a sketch curiosity--say the_but because\n",
      "\n",
      ">>> Experiment 2/5 with Topk=250 and Temperature=1.4 ...\n",
      "Every effort moves you knowbur never qualities from the ax had it beinteresting and chair, when\n",
      "\n",
      ">>> Experiment 3/5 with Topk=250 and Temperature=1.4 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes-- thing? I reallyinteresting with a single one\n",
      "\n",
      ">>> Experiment 4/5 with Topk=250 and Temperature=1.4 ...\n",
      "Every effort moves you say,\" was even do takend for nothingWhat a pale You the Sev\n",
      "\n",
      ">>> Experiment 5/5 with Topk=250 and Temperature=1.4 ...\n",
      "Every effort moves you know to stay and continued to affect! If Rickham, or twiceling\n",
      "CPU times: user 49.7 s, sys: 39.2 ms, total: 49.7 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Number of experiments to run for different top_k values.\n",
    "n_experiments = 5\n",
    "\n",
    "# Different top_k values to test.\n",
    "top_ks = [3, 25, 75, 250]\n",
    "temperature = 1.4\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for n_experiment in range(n_experiments):\n",
    "        print(\n",
    "            f\"\\n>>> Experiment {n_experiment + 1}/{n_experiments} with Topk={top_k} and Temperature={temperature} ...\"\n",
    "        )\n",
    "\n",
    "        # Generate text with the specified top_k.\n",
    "        token_ids = generate(\n",
    "            model=model,\n",
    "            idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "            max_new_tokens=15,\n",
    "            context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "            top_k=top_k,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Print the generated text.\n",
    "        print(f\"{token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Experiment 1/5 with Topk=5 and Temperature=0.0 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 2/5 with Topk=5 and Temperature=0.0 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 3/5 with Topk=5 and Temperature=0.0 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 4/5 with Topk=5 and Temperature=0.0 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 5/5 with Topk=5 and Temperature=0.0 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 1/5 with Topk=5 and Temperature=0.1 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 2/5 with Topk=5 and Temperature=0.1 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 3/5 with Topk=5 and Temperature=0.1 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 4/5 with Topk=5 and Temperature=0.1 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 5/5 with Topk=5 and Temperature=0.1 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 1/5 with Topk=5 and Temperature=0.3 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 2/5 with Topk=5 and Temperature=0.3 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 3/5 with Topk=5 and Temperature=0.3 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 4/5 with Topk=5 and Temperature=0.3 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 5/5 with Topk=5 and Temperature=0.3 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 1/5 with Topk=5 and Temperature=1.0 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. Gis\n",
      "\n",
      ">>> Experiment 2/5 with Topk=5 and Temperature=1.0 ...\n",
      "Every effort moves you know where to the furrowed by a smile that lifted the frame called\n",
      "\n",
      ">>> Experiment 3/5 with Topk=5 and Temperature=1.0 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 4/5 with Topk=5 and Temperature=1.0 ...\n",
      "Every effort moves you say to work on--as Mrs. Gisburn--as such--\n",
      "\n",
      ">>> Experiment 5/5 with Topk=5 and Temperature=1.0 ...\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n",
      "\n",
      ">>> Experiment 1/5 with Topk=5 and Temperature=3.0 ...\n",
      "Every effort moves you say to go on that, and for the were, struck with equanim\n",
      "\n",
      ">>> Experiment 2/5 with Topk=5 and Temperature=3.0 ...\n",
      "Every effort moves you say to put thelanded frame. Gis, and uncertain.\n",
      "\n",
      "\n",
      ">>> Experiment 3/5 with Topk=5 and Temperature=3.0 ...\n",
      "Every effort moves you?\"\n",
      "\"Oh, my too? I haven't--as such--\n",
      "\n",
      ">>> Experiment 4/5 with Topk=5 and Temperature=3.0 ...\n",
      "Every effort moves you in the work on the brush.\"\n",
      "\n",
      "\n",
      "I turned told--I\n",
      "\n",
      ">>> Experiment 5/5 with Topk=5 and Temperature=3.0 ...\n",
      "Every effort moves you say,\" she corrected--forming, he never to the deprecating of\n",
      "\n",
      ">>> Experiment 1/5 with Topk=5 and Temperature=10 ...\n",
      "Every effort moves you in the inevitable on that the picture--his face it all Graft background\n",
      "\n",
      ">>> Experiment 2/5 with Topk=5 and Temperature=10 ...\n",
      "Every effort moves you in spite to go that pushed wild for nothing with that told me to point\n",
      "\n",
      ">>> Experiment 3/5 with Topk=5 and Temperature=10 ...\n",
      "Every effort moves you thought: they should be that he seemed to the, a little his point\n",
      "\n",
      ">>> Experiment 4/5 with Topk=5 and Temperature=10 ...\n",
      "Every effort moves you in an to the a the frame heoms he--I didn to point\n",
      "\n",
      ">>> Experiment 5/5 with Topk=5 and Temperature=10 ...\n",
      "Every effort moves you thought it never up his pictures--I have the, a dep hisanim\n",
      "CPU times: user 1min 13s, sys: 23.3 ms, total: 1min 13s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Number of experiments to run for different top_k values.\n",
    "n_experiments = 5\n",
    "\n",
    "# Different top_k values to test.\n",
    "top_k = 5\n",
    "temperatures = [0.0, 0.1, 0.3, 1.0, 3.0, 10]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    for n_experiment in range(n_experiments):\n",
    "        print(\n",
    "            f\"\\n>>> Experiment {n_experiment + 1}/{n_experiments} with Topk={top_k} and Temperature={temperature} ...\"\n",
    "        )\n",
    "\n",
    "        # Generate text with the specified top_k.\n",
    "        token_ids = generate(\n",
    "            model=model,\n",
    "            idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "            max_new_tokens=15,\n",
    "            context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "            top_k=top_k,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Print the generated text.\n",
    "        print(f\"{token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Applying no temperature scaling;\n",
    "- Applying `top_k=1`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Saving Model Weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(256, 768)\n",
      "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 349 ms, sys: 322 ms, total: 670 ms\n",
      "Wall time: 5.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(model)\n",
    "torch.save(model.state_dict(), \"/llm_app/models/model_chapter5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = aux.GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/llm_app/models/model_chapter5.pth\", map_location=device, weights_only=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Disabling the dropout layers of the model.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 809 ms, sys: 1.15 s, total: 1.96 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"/llm_app/models/model_and_optimizer_chapter5.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 1.37 s, total: 2.9 s\n",
      "Wall time: 7.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    \"/llm_app/models/model_and_optimizer_chapter5.pth\",\n",
    "    map_location=device,\n",
    "    weights_only=True,\n",
    ")\n",
    "\n",
    "model = aux.GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch: 1 (Step 000000): Train loss 0.372, Validation loss 6.503\n",
      ">>> Epoch: 1 (Step 000005): Train loss 0.266, Validation loss 6.506\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=1,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x7fdda250dd00>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "\n",
    "filename = url.split(\"/\")[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:18:15.147819: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-11 13:18:15.151347: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-11 13:18:15.160370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744377495.175679     105 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744377495.180090     105 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744377495.191554     105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744377495.191567     105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744377495.191570     105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744377495.191571     105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-11 13:18:15.195292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 256,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      ">>> Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\">>> Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2-small (124M)': {'emb_dim': 768, 'n_layers': 12, 'n_heads': 12},\n",
       " 'gpt2-medium (355M)': {'emb_dim': 1024, 'n_layers': 24, 'n_heads': 16},\n",
       " 'gpt2-large (774M)': {'emb_dim': 1280, 'n_layers': 36, 'n_heads': 20},\n",
       " 'gpt2-xl (1558M)': {'emb_dim': 1600, 'n_layers': 48, 'n_heads': 25}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': True}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "\n",
    "NEW_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = aux.GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right, verbose=False):\n",
    "\n",
    "    if verbose:\n",
    "        print(f\">>> Types: Left is {type(left)}; Right is {type(right)}\")\n",
    "\n",
    "    if left.shape != right.shape:\n",
    "\n",
    "        raise ValueError(\n",
    "            f\">>> Shape mismatch. Left: {left.shape}, Right: {right.shape}\"\n",
    "        )\n",
    "\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params):\n",
    "\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T\n",
    "        )\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Output text:\n",
      " Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5,\n",
    ")\n",
    "print(\">>> Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = aux.create_dataloader_v1(\n",
    "    txt=train_data,\n",
    "    batch_size=2,\n",
    "    max_length=NEW_CONFIG[\"context_length\"] // 4,\n",
    "    stride=NEW_CONFIG[\"context_length\"] // 4,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = aux.create_dataloader_v1(\n",
    "    txt=val_data,\n",
    "    batch_size=2,\n",
    "    max_length=NEW_CONFIG[\"context_length\"] // 4,\n",
    "    stride=NEW_CONFIG[\"context_length\"] // 4,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Device: cpu\n",
      ">>> Training loss: 3.7547892994350858\n",
      ">>> Validation loss: 3.559669256210327\n",
      "CPU times: user 13 s, sys: 8.15 ms, total: 13.1 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\">>> Device:\", device)\n",
    "gpt.to(device)\n",
    "\n",
    "# For reproducibility due to the shuffling in the data loader.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Disable gradient tracking for efficiency because we are not training, yet.\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "    val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\">>> Training loss:\", train_loss)\n",
    "print(\">>> Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/1558M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/1558M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/1558M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/1558M/vocab.bpe\n",
      "CPU times: user 2.78 s, sys: 3.26 s, total: 6.04 s\n",
      "Wall time: 9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 1600,\n",
       " 'n_heads': 25,\n",
       " 'n_layers': 48,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': True}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2-xl (1558M)\"\n",
    "\n",
    "NEW_CONFIG2 = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG2.update(model_configs[model_name])\n",
    "NEW_CONFIG2.update({\"context_length\": 1024})\n",
    "NEW_CONFIG2.update({\"qkv_bias\": True})\n",
    "\n",
    "NEW_CONFIG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1600)\n",
       "  (pos_emb): Embedding(1024, 1600)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (24): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (25): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (26): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (27): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (28): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (29): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (30): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (31): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (32): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (33): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (34): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (35): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (36): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (37): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (38): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (39): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (40): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (41): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (42): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (43): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (44): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (45): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (46): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (47): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2 = aux.GPTModel(NEW_CONFIG2)\n",
    "gpt2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.01 s, sys: 3.72 s, total: 6.73 s\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "load_weights_into_gpt(gpt2, params)\n",
    "gpt2.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training loss: 3.3046699894799128\n",
      ">>> Validation loss: 3.119532585144043\n",
      "CPU times: user 2min 41s, sys: 30.9 s, total: 3min 12s\n",
      "Wall time: 48.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\">>> Device:\", device)\n",
    "gpt2.to(device)\n",
    "\n",
    "# For reproducibility due to the shuffling in the data loader.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Disable gradient tracking for efficiency because we are not training, yet.\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, gpt2, device)\n",
    "    val_loss = calc_loss_loader(val_loader, gpt2, device)\n",
    "\n",
    "print(\">>> Training loss:\", train_loss)\n",
    "print(\">>> Validation loss:\", val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
