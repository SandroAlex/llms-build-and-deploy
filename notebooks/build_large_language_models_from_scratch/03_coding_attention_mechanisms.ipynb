{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20921c83-1c1e-425e-bfda-c7b1b305eb2b",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a127bf-d75b-4430-b08a-52502142984b",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14555e42-c4b0-488b-9171-34356dcbef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.26 s, sys: 402 ms, total: 3.66 s\n",
      "Wall time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffb665-0680-48f6-b2f6-c856679fbea8",
   "metadata": {},
   "source": [
    "## Attending to Different Parts of the Input with Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5db59-23d1-4bb5-8fcb-55606457397d",
   "metadata": {},
   "source": [
    "### A Simple Self-Attention Mechanism Without Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8548adc-52a0-418e-bd49-4893930cfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "   [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "   [0.05, 0.80, 0.55],  # step     (x^6)\n",
    "   [-0.55 * 1, -0.87 * 1, -0.66 * 1]] # - 1 * x2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37fe00e-7108-447b-928c-833938660246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9544,  1.4950,  1.4754,  0.8434,  0.7070,  1.0865, -1.4950])\n"
     ]
    }
   ],
   "source": [
    "# The second input token is the query.\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b54c65-0fe8-4b3a-8a32-db3caa382292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([ 0.1884,  0.2951,  0.2912,  0.1665,  0.1395,  0.2144, -0.2951])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0369bce6-799d-4401-8cfa-f66aa4d391d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1369, 0.2351, 0.2305, 0.1225, 0.1069, 0.1562, 0.0118])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d5e6ef-2420-4970-97c0-51d69b2b5106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1369, 0.2351, 0.2305, 0.1225, 0.1069, 0.1562, 0.0118])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31edf80-92af-47f0-972f-6db66cef4f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4301, 0.6335, 0.5538])\n"
     ]
    }
   ],
   "source": [
    "# The second input token is the query.\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d46895-8353-46f6-b32e-96e58e9fc2e1",
   "metadata": {},
   "source": [
    "### Computing Attention Weights for All Input Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1526644a-39f7-4793-9c99-7e0b3a4decf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9995,  0.9544,  0.9422,  0.4753,  0.4576,  0.6310, -0.9544],\n",
      "        [ 0.9544,  1.4950,  1.4754,  0.8434,  0.7070,  1.0865, -1.4950],\n",
      "        [ 0.9422,  1.4754,  1.4570,  0.8296,  0.7154,  1.0605, -1.4754],\n",
      "        [ 0.4753,  0.8434,  0.8296,  0.4937,  0.3474,  0.6565, -0.8434],\n",
      "        [ 0.4576,  0.7070,  0.7154,  0.3474,  0.6654,  0.2935, -0.7070],\n",
      "        [ 0.6310,  1.0865,  1.0605,  0.6565,  0.2935,  0.9450, -1.0865],\n",
      "        [-0.9544, -1.4950, -1.4754, -0.8434, -0.7070, -1.0865,  1.4950]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6 + 1, 6 + 1)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a30039c-451c-4e8f-9f84-00e01e22b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9995,  0.9544,  0.9422,  0.4753,  0.4576,  0.6310, -0.9544],\n",
      "        [ 0.9544,  1.4950,  1.4754,  0.8434,  0.7070,  1.0865, -1.4950],\n",
      "        [ 0.9422,  1.4754,  1.4570,  0.8296,  0.7154,  1.0605, -1.4754],\n",
      "        [ 0.4753,  0.8434,  0.8296,  0.4937,  0.3474,  0.6565, -0.8434],\n",
      "        [ 0.4576,  0.7070,  0.7154,  0.3474,  0.6654,  0.2935, -0.7070],\n",
      "        [ 0.6310,  1.0865,  1.0605,  0.6565,  0.2935,  0.9450, -1.0865],\n",
      "        [-0.9544, -1.4950, -1.4754, -0.8434, -0.7070, -1.0865,  1.4950]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1de7d1-5e1c-4f4b-81f7-689a22372552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2038, 0.1948, 0.1924, 0.1206, 0.1185, 0.1410, 0.0289],\n",
      "        [0.1369, 0.2351, 0.2305, 0.1225, 0.1069, 0.1562, 0.0118],\n",
      "        [0.1373, 0.2340, 0.2298, 0.1227, 0.1094, 0.1545, 0.0122],\n",
      "        [0.1382, 0.1997, 0.1970, 0.1408, 0.1216, 0.1657, 0.0370],\n",
      "        [0.1457, 0.1869, 0.1885, 0.1305, 0.1793, 0.1236, 0.0455],\n",
      "        [0.1351, 0.2131, 0.2076, 0.1386, 0.0964, 0.1850, 0.0243],\n",
      "        [0.0587, 0.0342, 0.0349, 0.0656, 0.0752, 0.0514, 0.6800]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa7883c-da16-4bd4-b6e6-403bcf340f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6935cb51-54e5-4dea-82ad-d223a5d2f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: torch.Size([7, 7]) x torch.Size([7, 3])\n",
      "\n",
      "tensor([[ 0.4134,  0.5508,  0.5432],\n",
      "        [ 0.4301,  0.6335,  0.5538],\n",
      "        [ 0.4310,  0.6310,  0.5521],\n",
      "        [ 0.3941,  0.5744,  0.5063],\n",
      "        [ 0.4209,  0.5246,  0.4727],\n",
      "        [ 0.3943,  0.6134,  0.5348],\n",
      "        [-0.2352, -0.4254, -0.2942]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "\n",
    "print(f\"Shapes: {attn_weights.shape} x {inputs.shape}\")\n",
    "print()\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9360ec-ab20-4794-b0ec-2bdff0cfb1cd",
   "metadata": {},
   "source": [
    "## Implementing Self-Attention with Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050b9e3-9b3e-4ea1-8184-0449dd1c82be",
   "metadata": {},
   "source": [
    "### Computing the Attention Weights Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be55d045-e907-4e22-88ac-8ba7535d0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356c11c0-32ee-4d11-8b7a-c3388b7a78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7346163f-5c19-4363-9d3d-ceec80998638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68a59a7a-23e1-430a-bcea-0e2b5947b9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([7, 2])\n",
      "values.shape: torch.Size([7, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9daecf-0780-433c-a713-90e72cdab27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c9501a6-ac9e-4052-a3b8-7b669e4743d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2705,  1.8524,  1.8111,  1.0795,  0.5577,  1.5440, -1.8524])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcaf2987-db40-42de-85b6-dde9aedeac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1476, 0.2227, 0.2163, 0.1289, 0.0892, 0.1791, 0.0162])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim=-1)\n",
    "\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4897cbb6-4668-4dd9-9465-6ce8dfceac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2947, 0.7914])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810416b0-07c6-453b-9a61-4e8f26a913cf",
   "metadata": {},
   "source": [
    "### Implementing a Compact Self-Attention Python Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1edf5889-893d-49ba-a1ba-6788a29e3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    \"\"\"\n",
    "    Listing 3.1 A compact self-attention class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_in, d_out):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b83f6d0e-3a68-40bb-b85a-a58e94328043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2787,  0.7509],\n",
      "        [ 0.2947,  0.7914],\n",
      "        [ 0.2941,  0.7898],\n",
      "        [ 0.2636,  0.7126],\n",
      "        [ 0.2562,  0.6940],\n",
      "        [ 0.2772,  0.7471],\n",
      "        [-0.1392, -0.3375]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ab62039-e711-4d44-97c7-aaffa2e1918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    \"\"\"\n",
    "    Listing 3.2 A self-attention class using PyTorchâ€™s Linear layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1) \n",
    "        context_vec = attn_weights @ values\n",
    "    \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e62368e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4124, -0.0758],\n",
      "        [-0.3977, -0.0751],\n",
      "        [-0.3977, -0.0751],\n",
      "        [-0.3710, -0.0688],\n",
      "        [-0.3857, -0.0713],\n",
      "        [-0.3729, -0.0697],\n",
      "        [-0.2928, -0.0484]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c035486",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Comparing Self-Attention v1 and V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "854ef00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3035, 0.1430],\n",
      "        [0.2954, 0.1392],\n",
      "        [0.2955, 0.1392],\n",
      "        [0.3274, 0.1542],\n",
      "        [0.3163, 0.1491],\n",
      "        [0.3236, 0.1524],\n",
      "        [0.4059, 0.1912]], grad_fn=<MmBackward0>)\n",
      "\n",
      "tensor([[0.3035, 0.1430],\n",
      "        [0.2954, 0.1392],\n",
      "        [0.2955, 0.1392],\n",
      "        [0.3274, 0.1542],\n",
      "        [0.3163, 0.1491],\n",
      "        [0.3236, 0.1524],\n",
      "        [0.4059, 0.1912]], grad_fn=<MmBackward0>)\n",
      "\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the self-attention classes.\n",
    "sav1 = SelfAttention_v1(d_in, d_out)\n",
    "sav2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "# Copy the weights from the second to the first class.\n",
    "sav1.W_key.data = sav2.W_key.weight.data.T\n",
    "sav1.W_query.data = sav2.W_query.weight.data.T\n",
    "sav1.W_value.data = sav2.W_value.weight.data.T\n",
    "\n",
    "# Compare if the two classes produce the same output.\n",
    "print(sav1(inputs))\n",
    "print()\n",
    "print(sav2(inputs))\n",
    "print()\n",
    "print(sav1(inputs) - sav2(inputs))  # Should be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff789e",
   "metadata": {},
   "source": [
    "## Hiding Future Words with Causal Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27e074",
   "metadata": {},
   "source": [
    "### Applying a Causal Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9214dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1550, 0.1591, 0.1590, 0.1404, 0.1468, 0.1425, 0.0972],\n",
      "        [0.1459, 0.1560, 0.1558, 0.1438, 0.1432, 0.1473, 0.1080],\n",
      "        [0.1460, 0.1560, 0.1557, 0.1437, 0.1432, 0.1473, 0.1080],\n",
      "        [0.1428, 0.1486, 0.1485, 0.1441, 0.1424, 0.1461, 0.1276],\n",
      "        [0.1473, 0.1521, 0.1520, 0.1429, 0.1442, 0.1448, 0.1167],\n",
      "        [0.1420, 0.1493, 0.1491, 0.1445, 0.1420, 0.1470, 0.1262],\n",
      "        [0.1380, 0.1290, 0.1293, 0.1400, 0.1406, 0.1367, 0.1864]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bad1d9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0dd0110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "212f8822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1550, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1459, 0.1560, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1460, 0.1560, 0.1557, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1428, 0.1486, 0.1485, 0.1441, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1473, 0.1521, 0.1520, 0.1429, 0.1442, 0.0000, 0.0000],\n",
      "        [0.1420, 0.1493, 0.1491, 0.1445, 0.1420, 0.1470, 0.0000],\n",
      "        [0.1380, 0.1290, 0.1293, 0.1400, 0.1406, 0.1367, 0.1864]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af49dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682, 0.0000],\n",
      "        [0.1380, 0.1290, 0.1293, 0.1400, 0.1406, 0.1367, 0.1864]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "print(row_sums.shape)\n",
    "\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62e41005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple_norm.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3949b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b7a6ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3111,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1655,  0.2602,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1667,  0.2602,  0.2577,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0510,  0.1080,  0.1064,  0.0643,    -inf,    -inf,    -inf],\n",
       "        [ 0.1415,  0.1875,  0.1863,  0.0987,  0.1121,    -inf,    -inf],\n",
       "        [ 0.0476,  0.1192,  0.1171,  0.0731,  0.0477,  0.0966,    -inf],\n",
       "        [-0.1655, -0.2602, -0.2576, -0.1445, -0.1384, -0.1790,  0.2602]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28b3780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682, 0.0000],\n",
      "        [0.1380, 0.1290, 0.1293, 0.1400, 0.1406, 0.1367, 0.1864]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5718517d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eafdb6",
   "metadata": {},
   "source": [
    "### Masking Additional Attention Weights with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79468b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2., 0.],\n",
      "        [2., 0., 0., 0., 0., 0., 0.],\n",
      "        [2., 0., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 2., 2., 0., 0., 0.],\n",
      "        [0., 2., 0., 2., 0., 0., 0.],\n",
      "        [0., 0., 0., 2., 2., 2., 0.],\n",
      "        [0., 2., 0., 0., 2., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "dropout = torch.nn.Dropout(1/2)\n",
    "example = torch.ones(7, 7)\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "963d128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682, 0.0000],\n",
      "        [0.1380, 0.1290, 0.1293, 0.1400, 0.1406, 0.1367, 0.1864]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9665, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6380, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5085, 0.4936, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4120, 0.0000, 0.3869, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3308, 0.3249, 0.3363, 0.0000],\n",
      "        [0.0000, 0.2581, 0.0000, 0.0000, 0.2813, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "print(attn_weights)\n",
    "print()\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260e23a",
   "metadata": {},
   "source": [
    "### Implementing a Compact Causal Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8d0cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19791e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Listing 3.3: A compact causal attention class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            name=\"mask\",\n",
    "            tensor=torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "       \n",
    "        # In PyTorch, operations with a trailing underscore are performed \n",
    "        # inplace, avoiding unnecessary memory copies.\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)   \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab095d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 7, 3])\n",
      "context_vecs.shape: torch.Size([2, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(\"batch.shape:\", batch.shape)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeacc07",
   "metadata": {},
   "source": [
    "## Extending Single-Head Attention to Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87162aa",
   "metadata": {},
   "source": [
    "### Stacking Multiple Single-Head Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c8f992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Listing 3.4: A wrapper class to implement multi-head attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return torch.cat(\n",
    "            [head(x) for head in self.heads], dim=-1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "101df023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493],\n",
      "         [-0.2928, -0.0484,  0.3210,  0.2033]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493],\n",
      "         [-0.2928, -0.0484,  0.3210,  0.2033]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens.\n",
    "\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe09d15",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Returning Two-Dimensional Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43c87be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5740,  0.2216],\n",
      "         [-0.7320,  0.0155],\n",
      "         [-0.7774, -0.0546],\n",
      "         [-0.6979, -0.0817],\n",
      "         [-0.6538, -0.0957],\n",
      "         [-0.6424, -0.1065],\n",
      "         [-0.4246, -0.0681]],\n",
      "\n",
      "        [[-0.5740,  0.2216],\n",
      "         [-0.7320,  0.0155],\n",
      "         [-0.7774, -0.0546],\n",
      "         [-0.6979, -0.0817],\n",
      "         [-0.6538, -0.0957],\n",
      "         [-0.6424, -0.1065],\n",
      "         [-0.4246, -0.0681]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens.\n",
    "\n",
    "d_in, d_out = 3, 1\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d7944",
   "metadata": {},
   "source": [
    "### Implementing Multi-Head Attention with Weight Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04b5e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Listing 3.5 An efficient multi-head attention class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            f\"d_out ({d_out}) must be divisible by num_heads ({num_heads})!\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Reduces the projection dim to match the desired output dim.\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # Uses a Linear layer to combine head outputs.\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            name=\"mask\",\n",
    "            tensor=torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # We implicitly split the matrix by adding a num_heads dimension.\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transposes: \n",
    "        #   from  (b, num_tokens, num_heads, head_dim) \n",
    "        #   to    (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Computes dot product for each head.\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        # Masks truncated to the number of tokens.\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        # Uses the mask to fill attention scores.\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "    \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Tensor shape: (b, num_tokens, n_heads, head_dim).\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combines heads, where self.d_out = self.num_heads * self.head_dim.\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        # Adds an optional linear projection.\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dae3e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4).\n",
    "a = torch.tensor(\n",
    "    [[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "       [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "       [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "      [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "       [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "       [0.4606, 0.5159, 0.4220, 0.5786]\n",
    "    ]]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "292598ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9620b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n",
      "torch.Size([1, 2, 4, 3])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "b = a @ a.transpose(2, 3)\n",
    "print(a.shape)\n",
    "print(a.transpose(2, 3).shape)\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e46ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7b08b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028],\n",
      "         [0.2232, 0.5420]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028],\n",
      "         [0.2232, 0.5420]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb9ba4",
   "metadata": {},
   "source": [
    "#### Exercise 3.3: Initializing GPT-2 Size Attention Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5a8c3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([1, 1024, 768])\n",
      "CPU times: user 385 ms, sys: 121 ms, total: 506 ms\n",
      "Wall time: 326 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size = 1; context_length = 1024; d_in = 768; d_out = 768; num_heads = 12; dropout = 0.0\n",
    "\n",
    "size = (batch_size, context_length, d_in)\n",
    "params = dict(d_in=d_in, d_out=d_out, context_length=context_length, dropout=dropout, num_heads=num_heads)\n",
    "\n",
    "batch = torch.empty(size=size)\n",
    "mha = MultiHeadAttention(**params)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
